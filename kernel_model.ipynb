{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b2e6273d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-12T08:19:10.189500Z",
          "start_time": "2025-11-12T08:19:08.227573Z"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict\n",
        "from types import SimpleNamespace\n",
        "from itertools import combinations\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_curve, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.ndimage import binary_fill_holes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56d7e08e",
      "metadata": {},
      "source": [
        "--------------------------- Utilities ---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ed5117fb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-12T08:19:10.206231Z",
          "start_time": "2025-11-12T08:19:10.195133Z"
        }
      },
      "outputs": [],
      "source": [
        "def mask_from_green_contour(annot_img, GREEN_THRESH):\n",
        "    \"\"\"\n",
        "    Returns a binary mask where tumor area (green contour) = 1\n",
        "    \"\"\"\n",
        "    r,g,b = cv2.split(annot_img)\n",
        "    # mask = ((g < 140) & (r < 50) & ( > 190)).astype(np.uint8) #  Green\n",
        "    mask = ((g < 70) & (b < 70) & (r > 150)).astype(np.uint8)  # Red\n",
        "\n",
        "    # fill holes\n",
        "    mask = cv2.dilate(mask, np.ones((3,3), np.uint8), iterations=2)\n",
        "    mask = cv2.erode(mask, np.ones((3,3), np.uint8), iterations=2)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = binary_fill_holes(mask).astype(np.uint8)\n",
        "    mask = cv2.cvtColor(mask*255, cv2.COLOR_GRAY2BGR)[:,:,0] > 0\n",
        "\n",
        "    return mask\n",
        "\n",
        "def sample_patches(img, mask, patch_size, n_inside, n_outside, max_tries=500):\n",
        "    H,W = img.shape[:2]\n",
        "    inside_patches, outside_patches = [], []\n",
        "\n",
        "    # inside\n",
        "    if mask is not None:\n",
        "        tries = 0\n",
        "        while len(inside_patches) < n_inside and tries < max_tries:\n",
        "            x = random.randint(0, W - patch_size)\n",
        "            y = random.randint(0, H - patch_size)\n",
        "            patch_mask = mask[y:y+patch_size, x:x+patch_size]\n",
        "            # require at least 80% inside mask\n",
        "            if patch_mask.mean() > 0.8:\n",
        "                patch = img[y:y+patch_size, x:x+patch_size]\n",
        "                inside_patches.append(patch)\n",
        "            tries += 1\n",
        "    else:\n",
        "        mask = np.zeros((*img.shape[:2], 3), dtype=np.uint8)\n",
        "\n",
        "    # outside\n",
        "    if img is not None:\n",
        "        tries = 0\n",
        "        while len(outside_patches) < n_outside and tries < max_tries:\n",
        "            x = random.randint(0, W - patch_size)\n",
        "            y = random.randint(0, H - patch_size)\n",
        "            patch_mask = mask[y:y+patch_size, x:x+patch_size]\n",
        "            # require <5% overlap with tumor\n",
        "            if patch_mask.mean() < 0.05:\n",
        "                patch = img[y:y+patch_size, x:x+patch_size]\n",
        "                outside_patches.append(patch)\n",
        "            tries += 1\n",
        "\n",
        "    return inside_patches, outside_patches\n",
        "\n",
        "\n",
        "def load_patches_from_folders(root_dir, max_per_class=None, gray=True, resize=None):\n",
        "    \"\"\"Expect two folders inside root_dir: 'inside' and 'outside'.\"\"\"\n",
        "    inside_dir = Path(root_dir) / 'inside'\n",
        "    outside_dir = Path(root_dir) / 'outside'\n",
        "    os.makedirs(inside_dir, exist_ok=True)\n",
        "    os.makedirs(outside_dir, exist_ok=True)\n",
        "\n",
        "    assert inside_dir.exists() and outside_dir.exists(), \"Folders 'inside' and 'outside' required\"\n",
        "\n",
        "    def load_from(folder, limit):\n",
        "        files = sorted([p for p in folder.iterdir() if p.suffix.lower() in ('.tif','.jpg','.jpeg','.tif','.tiff')])\n",
        "        if limit:\n",
        "            files = files[:limit]\n",
        "        imgs = []\n",
        "        for p in files:\n",
        "            im = Image.open(p)\n",
        "            if gray:\n",
        "                im = im.convert('L')\n",
        "            else:\n",
        "                im = im.convert('RGB')\n",
        "            if resize:\n",
        "                im = im.resize(resize, Image.BILINEAR)\n",
        "            arr = np.array(im, dtype=np.float32) / 255.0\n",
        "            imgs.append(arr)\n",
        "        return np.stack(imgs, axis=0) if imgs else np.zeros((0, *(resize if resize else im.size[::-1])), dtype=np.float32)\n",
        "\n",
        "    Xin = load_from(inside_dir, max_per_class)\n",
        "    Xout = load_from(outside_dir, max_per_class)\n",
        "    # ensure shape N,H,W\n",
        "    return Xin, Xout\n",
        "\n",
        "def to_tensor_batch(X, device):\n",
        "    # X: N,H,W -> return tensor N,1,H,W\n",
        "    t = torch.from_numpy(X).unsqueeze(1).to(device)\n",
        "    return t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a043b6",
      "metadata": {},
      "source": [
        "--------------------------- Kernel generators ---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a97d4838",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-12T08:19:10.253702Z",
          "start_time": "2025-11-12T08:19:10.248557Z"
        }
      },
      "outputs": [],
      "source": [
        "def make_gaussian_kernel(size, sigma_x, sigma_y=None, theta=0.0):\n",
        "    if sigma_y is None:\n",
        "        sigma_y = sigma_x\n",
        "    assert size % 2 == 1, 'size should be odd'\n",
        "    half = size // 2\n",
        "    xs = np.arange(-half, half+1, 1)\n",
        "    ys = np.arange(-half, half+1, 1)\n",
        "    X, Y = np.meshgrid(xs, ys)\n",
        "    # rotate\n",
        "    ct = math.cos(theta); st = math.sin(theta)\n",
        "    Xr = ct * X + st * Y\n",
        "    Yr = -st * X + ct * Y\n",
        "    G = np.exp(-0.5 * ((Xr**2) / (sigma_x**2 + 1e-12) + (Yr**2) / (sigma_y**2 + 1e-12)))\n",
        "    G = G / (G.sum() + 1e-12)\n",
        "    return G.astype(np.float32)\n",
        "\n",
        "\n",
        "def make_dog_kernel(size, sigma1, sigma2, theta=0.0):\n",
        "    g1 = make_gaussian_kernel(size, sigma1, sigma1, theta)\n",
        "    g2 = make_gaussian_kernel(size, sigma2, sigma2, theta)\n",
        "    k = g1 - g2\n",
        "    # normalize zero mean\n",
        "    k = k - k.mean()\n",
        "    return k.astype(np.float32)\n",
        "\n",
        "\n",
        "def make_log_kernel(size, sigma, theta=0.0):\n",
        "    # Laplacian of Gaussian approximation\n",
        "    assert size % 2 == 1\n",
        "    half = size // 2\n",
        "    xs = np.arange(-half, half+1, 1)\n",
        "    ys = np.arange(-half, half+1, 1)\n",
        "    X, Y = np.meshgrid(xs, ys)\n",
        "    ct = math.cos(theta); st = math.sin(theta)\n",
        "    Xr = ct * X + st * Y\n",
        "    Yr = -st * X + ct * Y\n",
        "    r2 = (Xr**2 + Yr**2)\n",
        "    s2 = sigma**2\n",
        "    LoG = ((r2 - 2*s2) / (s2**2)) * np.exp(-r2/(2*s2))\n",
        "    LoG = LoG - LoG.mean()\n",
        "    return LoG.astype(np.float32)\n",
        "\n",
        "\n",
        "def make_gabor_kernel(size, sigma, freq, theta=0.0, phase=0.0, gamma=1.0):\n",
        "    # gamma: aspect ratio\n",
        "    assert size % 2 == 1\n",
        "    half = size // 2\n",
        "    xs = np.arange(-half, half+1, 1)\n",
        "    ys = np.arange(-half, half+1, 1)\n",
        "    X, Y = np.meshgrid(xs, ys)\n",
        "    ct = math.cos(theta); st = math.sin(theta)\n",
        "    Xr = ct * X + st * Y\n",
        "    Yr = -st * X + ct * Y\n",
        "    Yr = Yr * gamma\n",
        "    gaussian = np.exp(-(Xr**2 + Yr**2) / (2 * (sigma**2)))\n",
        "    sinusoid = np.cos(2 * np.pi * freq * Xr + phase)\n",
        "    K = gaussian * sinusoid\n",
        "    # zero mean\n",
        "    K = K - K.mean()\n",
        "    # normalize by L1\n",
        "    if K.sum() != 0:\n",
        "        K = K / (np.abs(K).sum() + 1e-12)\n",
        "    return K.astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be043854",
      "metadata": {},
      "source": [
        "--------------------------- Sampling parameters ---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "4e063afe",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-12T08:19:10.304732Z",
          "start_time": "2025-11-12T08:19:10.300694Z"
        }
      },
      "outputs": [],
      "source": [
        "def sample_parameters(family, n_samples, size):\n",
        "    params = []\n",
        "    for _ in range(n_samples):\n",
        "        if family == 'gaussian':\n",
        "            sigma = float(10 ** np.random.uniform(np.log10(0.5), np.log10(size/2)))\n",
        "            theta = np.random.uniform(0, math.pi)\n",
        "            params.append({'sigma_x': sigma, 'sigma_y': sigma, 'theta': theta, 'size': size})\n",
        "        elif family == 'anisotropic_gaussian':\n",
        "            sigma_x = float(10 ** np.random.uniform(np.log10(0.5), np.log10(size/2)))\n",
        "            sigma_y = float(sigma_x * np.random.uniform(0.5, 3.0))\n",
        "            theta = np.random.uniform(0, math.pi)\n",
        "            params.append({'sigma_x': sigma_x, 'sigma_y': sigma_y, 'theta': theta, 'size': size})\n",
        "        elif family == 'dog':\n",
        "            s1 = float(np.random.uniform(0.5, size/2))\n",
        "            s2 = float(s1 * np.random.uniform(1.2, 3.0))\n",
        "            theta = np.random.uniform(0, math.pi)\n",
        "            params.append({'sigma1': s1, 'sigma2': s2, 'theta': theta, 'size': size})\n",
        "        elif family == 'log':\n",
        "            s = float(np.random.uniform(0.5, size/2))\n",
        "            theta = np.random.uniform(0, math.pi)\n",
        "            params.append({'sigma': s, 'theta': theta, 'size': size})\n",
        "        elif family == 'gabor':\n",
        "            sigma = float(np.random.uniform(0.5, size/2))\n",
        "            freq = float(np.random.uniform(0.02, 0.5))\n",
        "            theta = np.random.uniform(0, math.pi)\n",
        "            phase = float(np.random.uniform(0, 2*math.pi))\n",
        "            gamma = float(np.random.uniform(0.5, 1.5))\n",
        "            params.append({'sigma': sigma, 'freq': freq, 'theta': theta, 'phase': phase, 'gamma': gamma, 'size': size})\n",
        "        else:\n",
        "            raise ValueError('Unknown family')\n",
        "    return params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17c12c3e",
      "metadata": {},
      "source": [
        "--------------------------- Kernel bank builder ---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "93136a57",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-12T08:19:10.354440Z",
          "start_time": "2025-11-12T08:19:10.351571Z"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_kernel_bank(families, n_per_family, size):\n",
        "    bank = []\n",
        "    for fam in families:\n",
        "        params = sample_parameters(fam, n_per_family, size)\n",
        "        for p in params:\n",
        "            if fam == 'gaussian' or fam == 'anisotropic_gaussian':\n",
        "                k = make_gaussian_kernel(p['size'], p['sigma_x'], p.get('sigma_y', None), p['theta'])\n",
        "            elif fam == 'dog':\n",
        "                k = make_dog_kernel(p['size'], p['sigma1'], p['sigma2'], p['theta'])\n",
        "            elif fam == 'log':\n",
        "                k = make_log_kernel(p['size'], p['sigma'], p['theta'])\n",
        "            elif fam == 'gabor':\n",
        "                k = make_gabor_kernel(p['size'], p['sigma'], p['freq'], p['theta'], p['phase'], p['gamma'])\n",
        "            else:\n",
        "                continue\n",
        "            bank.append({'family': fam, 'params': p, 'kernel': k})\n",
        "    return bank"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5494e3e",
      "metadata": {},
      "source": [
        "--------------------------- Response computation ---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "c68726a4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-12T08:19:10.408475Z",
          "start_time": "2025-11-12T08:19:10.404194Z"
        }
      },
      "outputs": [],
      "source": [
        "def compute_responses(bank, X_in, X_out, device='cpu', batch_size=64, response_fn='abs_max'):\n",
        "    device = torch.device(device)\n",
        "    Xin_t = to_tensor_batch(X_in, device)\n",
        "    Xout_t = to_tensor_batch(X_out, device)\n",
        "    responses = []\n",
        "    # convert bank kernels to torch tensors (filters)\n",
        "    filter_tensors = []\n",
        "    for entry in bank:\n",
        "        k = entry['kernel']\n",
        "        k_t = torch.from_numpy(k).unsqueeze(0).unsqueeze(0).to(device)  # 1,1,Hk,Wk\n",
        "        filter_tensors.append(k_t)\n",
        "    # compute responses per kernel\n",
        "    for idx, k_t in enumerate(tqdm(filter_tensors, desc='Kernels')):\n",
        "        # conv Xin\n",
        "        r_in = []\n",
        "        for i in range(0, Xin_t.shape[0], batch_size):\n",
        "            batch = Xin_t[i:i+batch_size]\n",
        "            with torch.no_grad():\n",
        "                out = F.conv2d(batch, k_t, padding=k_t.shape[-1]//2)\n",
        "                if response_fn == 'abs_max':\n",
        "                    val = out.abs().amax(dim=[1,2,3]).cpu().numpy()\n",
        "                elif response_fn == 'mean_abs':\n",
        "                    val = out.abs().mean(dim=[1,2,3]).cpu().numpy()\n",
        "                else:\n",
        "                    val = out.abs().amax(dim=[1,2,3]).cpu().numpy()\n",
        "                r_in.append(val)\n",
        "        r_in = np.concatenate(r_in, axis=0) if len(r_in) else np.zeros((0,))\n",
        "\n",
        "        r_out = []\n",
        "        for i in range(0, Xout_t.shape[0], batch_size):\n",
        "            batch = Xout_t[i:i+batch_size]\n",
        "            with torch.no_grad():\n",
        "                out = F.conv2d(batch, k_t, padding=k_t.shape[-1]//2)\n",
        "                if response_fn == 'abs_max':\n",
        "                    val = out.abs().amax(dim=[1,2,3]).cpu().numpy()\n",
        "                elif response_fn == 'mean_abs':\n",
        "                    val = out.abs().mean(dim=[1,2,3]).cpu().numpy()\n",
        "                else:\n",
        "                    val = out.abs().amax(dim=[1,2,3]).cpu().numpy()\n",
        "                r_out.append(val)\n",
        "        r_out = np.concatenate(r_out, axis=0) if len(r_out) else np.zeros((0,))\n",
        "\n",
        "        responses.append({'r_in': r_in, 'r_out': r_out})\n",
        "    return responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "75f09f9d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-12T08:19:10.454629Z",
          "start_time": "2025-11-12T08:19:10.451494Z"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "def fisher_score(r_in, r_out):\n",
        "    mu_in = r_in.mean() if r_in.size else 0.0\n",
        "    mu_out = r_out.mean() if r_out.size else 0.0\n",
        "    var_in = r_in.var(ddof=1) if r_in.size>1 else 0.0\n",
        "    var_out = r_out.var(ddof=1) if r_out.size>1 else 0.0\n",
        "    num = (mu_in - mu_out)**2\n",
        "    den = var_in + var_out + 1e-12\n",
        "    return float(num / den)\n",
        "\n",
        "\n",
        "def auc_score(r_in, r_out):\n",
        "    y_true = np.concatenate([np.ones(len(r_in)), np.zeros(len(r_out))])\n",
        "    y_score = np.concatenate([r_in, r_out])\n",
        "    # if constant, AUC undefined -> return 0.5\n",
        "    try:\n",
        "        if y_score.max() == y_score.min():\n",
        "            return 0.5\n",
        "        return float(roc_auc_score(y_true, y_score))\n",
        "    except Exception:\n",
        "        return 0.5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "2eca35d0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-12T08:19:10.508286Z",
          "start_time": "2025-11-12T08:19:10.503694Z"
        }
      },
      "outputs": [],
      "source": [
        "def pairwise_response_corr(resp_i, resp_j):\n",
        "    # responses already 1D arrays on combined patches\n",
        "    a = resp_i - resp_i.mean()\n",
        "    b = resp_j - resp_j.mean()\n",
        "    denom = (np.linalg.norm(a) * np.linalg.norm(b) + 1e-12)\n",
        "    return float(np.dot(a,b) / denom)\n",
        "\n",
        "\n",
        "def select_diverse(bank, responses, topM=200, K=20, lambda_mm=0.75):\n",
        "    # rank by combined score (we'll use AUC primarily then fisher)\n",
        "    scores = []\n",
        "    for i, resp in enumerate(responses):\n",
        "        auc = auc_score(resp['r_in'], resp['r_out'])\n",
        "        fisher = fisher_score(resp['r_in'], resp['r_out'])\n",
        "        scores.append({'idx': i, 'auc': auc, 'fisher': fisher})\n",
        "    scores = sorted(scores, key=lambda x: (x['auc'], x['fisher']), reverse=True)\n",
        "    top_idxs = [s['idx'] for s in scores[:topM]]\n",
        "    # precompute combined response vectors\n",
        "    combined = []\n",
        "    for i in top_idxs:\n",
        "        combined.append(np.concatenate([responses[i]['r_in'], responses[i]['r_out']]))\n",
        "    selected = []\n",
        "    selected_idxs = []\n",
        "    # greedy MMR\n",
        "    for rank_i, idx in enumerate(top_idxs):\n",
        "        if not selected:\n",
        "            selected.append(idx)\n",
        "            selected_idxs.append(idx)\n",
        "            if len(selected) >= K:\n",
        "                break\n",
        "            continue\n",
        "        # compute mmr score for remaining\n",
        "        best_score = -1e9\n",
        "        best_idx = None\n",
        "        for j, cand in enumerate(top_idxs):\n",
        "            if cand in selected_idxs:\n",
        "                continue\n",
        "            auc_j = next(s for s in scores if s['idx']==cand)['auc']\n",
        "            # similarity to selected (max)\n",
        "            sims = [abs(pairwise_response_corr(combined[top_idxs.index(cand)], combined[top_idxs.index(s)])) for s in selected]\n",
        "            maxsim = max(sims) if sims else 0.0\n",
        "            mmr = lambda_mm * auc_j - (1-lambda_mm) * maxsim\n",
        "            if mmr > best_score:\n",
        "                best_score = mmr\n",
        "                best_idx = cand\n",
        "        if best_idx is None:\n",
        "            break\n",
        "        selected.append(best_idx)\n",
        "        selected_idxs.append(best_idx)\n",
        "        if len(selected) >= K:\n",
        "            break\n",
        "    return selected_idxs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bbb23bc",
      "metadata": {},
      "source": [
        "--------------------------- Training classifier ---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "0eeb730f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-12T08:19:10.561535Z",
          "start_time": "2025-11-12T08:19:10.555661Z"
        }
      },
      "outputs": [],
      "source": [
        "class SimpleLogistic(torch.nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.lin = torch.nn.Linear(n_features, 1)\n",
        "    def forward(self, x):\n",
        "        return self.lin(x).squeeze(1)\n",
        "\n",
        "\n",
        "class SimpleMLP(torch.nn.Module):\n",
        "    def __init__(self, n_features, hidden_dims=(64, 32), dropout=0.2):\n",
        "        super().__init__()\n",
        "        h1, h2 = hidden_dims\n",
        "        self.net = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_features, h1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            torch.nn.Linear(h1, h2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            torch.nn.Linear(h2, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "\n",
        "def standardize_split(X_tr, X_val):\n",
        "    mean = X_tr.mean(axis=0, keepdims=True)\n",
        "    std = X_tr.std(axis=0, keepdims=True)\n",
        "    std[std < 1e-6] = 1.0\n",
        "    return (X_tr - mean) / std, (X_val - mean) / std\n",
        "\n",
        "\n",
        "def train_binary_model(\n",
        "    X_feat,\n",
        "    y,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    lr=1e-3,\n",
        "    device='cpu',\n",
        "    return_model=True,\n",
        "    model_type='logistic',\n",
        "    hidden_dims=(64, 32),\n",
        "    dropout=0.2,\n",
        "    standardize=True\n",
        "):\n",
        "    if X_feat.size == 0 or y.size == 0:\n",
        "        return {'model': None, 'auc': 0.5, 'acc': 0.5}\n",
        "    device = torch.device(device)\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(X_feat, y, test_size=0.2, stratify=y, random_state=42)\n",
        "    if standardize:\n",
        "        X_tr, X_val = standardize_split(X_tr, X_val)\n",
        "\n",
        "    tr_ds = TensorDataset(torch.from_numpy(X_tr).float().to(device), torch.from_numpy(y_tr).float().to(device))\n",
        "    val_ds = TensorDataset(torch.from_numpy(X_val).float().to(device), torch.from_numpy(y_val).float().to(device))\n",
        "    tr_loader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    if model_type == 'mlp':\n",
        "        model = SimpleMLP(X_feat.shape[1], hidden_dims=hidden_dims, dropout=dropout).to(device)\n",
        "    else:\n",
        "        model = SimpleLogistic(X_feat.shape[1]).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    best_auc = 0.0\n",
        "    best_state = None\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in tr_loader:\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "        model.eval()\n",
        "        ys = []\n",
        "        ps = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                logits = model(xb)\n",
        "                probs = torch.sigmoid(logits).cpu().numpy()\n",
        "                ys.append(yb.cpu().numpy())\n",
        "                ps.append(probs)\n",
        "        ys = np.concatenate(ys)\n",
        "        ps = np.concatenate(ps)\n",
        "        try:\n",
        "            auc = roc_auc_score(ys, ps)\n",
        "        except Exception:\n",
        "            auc = 0.5\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_state = model.state_dict()\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    model.eval()\n",
        "    X_val_t = torch.from_numpy(X_val).float().to(device)\n",
        "    with torch.no_grad():\n",
        "        probs = torch.sigmoid(model(X_val_t)).cpu().numpy()\n",
        "    acc = accuracy_score(y_val, (probs>0.5).astype(int))\n",
        "    auc = roc_auc_score(y_val, probs)\n",
        "    return {\n",
        "        'model': model if return_model else None,\n",
        "        'auc': auc,\n",
        "        'acc': acc,\n",
        "        'val_probs': probs,\n",
        "        'val_labels': y_val\n",
        "    }\n",
        "\n",
        "\n",
        "def build_feature_matrix(selected_idxs, responses):\n",
        "    feats = []\n",
        "    for idx in selected_idxs:\n",
        "        resp = responses[idx]\n",
        "        r = np.concatenate([resp['r_in'], resp['r_out']])\n",
        "        feats.append(r.reshape(-1, 1))\n",
        "    if not feats:\n",
        "        return np.zeros((0, 0)), np.array([])\n",
        "    X_feat = np.concatenate(feats, axis=1)\n",
        "    n_in = len(responses[0]['r_in']) if responses else 0\n",
        "    n_out = len(responses[0]['r_out']) if responses else 0\n",
        "    y = np.concatenate([np.ones(n_in), np.zeros(n_out)]) if (n_in or n_out) else np.array([])\n",
        "    return X_feat, y\n",
        "\n",
        "\n",
        "def rank_kernels(responses):\n",
        "    scores = []\n",
        "    for i, resp in enumerate(responses):\n",
        "        scores.append({'idx': i, 'auc': auc_score(resp['r_in'], resp['r_out']), 'fisher': fisher_score(resp['r_in'], resp['r_out'])})\n",
        "    scores.sort(key=lambda x: (x['auc'], x['fisher']), reverse=True)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def find_best_subsets(\n",
        "    X_feat,\n",
        "    y,\n",
        "    subset_sizes,\n",
        "    epochs=15,\n",
        "    batch_size=64,\n",
        "    lr=1e-3,\n",
        "    device='cpu',\n",
        "    model_type='logistic',\n",
        "    hidden_dims=(64, 32),\n",
        "    dropout=0.2,\n",
        "    standardize=True\n",
        "):\n",
        "    best = {}\n",
        "    n_features = X_feat.shape[1]\n",
        "    for k in subset_sizes:\n",
        "        if n_features < k:\n",
        "            continue\n",
        "        best_entry = None\n",
        "        for combo in combinations(range(n_features), k):\n",
        "            res = train_binary_model(\n",
        "                X_feat[:, combo],\n",
        "                y,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                lr=lr,\n",
        "                device=device,\n",
        "                return_model=False,\n",
        "                model_type=model_type,\n",
        "                hidden_dims=hidden_dims,\n",
        "                dropout=dropout,\n",
        "                standardize=standardize\n",
        "            )\n",
        "            entry = {'subset': combo, 'auc': res['auc'], 'acc': res['acc']}\n",
        "            if best_entry is None or entry['auc'] > best_entry['auc']:\n",
        "                best_entry = entry\n",
        "        if best_entry:\n",
        "            best[k] = best_entry\n",
        "    return best\n",
        "\n",
        "\n",
        "def fit_subset_model(\n",
        "    X_feat,\n",
        "    y,\n",
        "    subset,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    lr=1e-3,\n",
        "    device='cpu',\n",
        "    model_type='logistic',\n",
        "    hidden_dims=(64, 32),\n",
        "    dropout=0.2,\n",
        "    standardize=True\n",
        "):\n",
        "    X_sub = X_feat[:, subset]\n",
        "    mean = std = None\n",
        "    if standardize:\n",
        "        mean = X_sub.mean(axis=0, keepdims=True)\n",
        "        std = X_sub.std(axis=0, keepdims=True)\n",
        "        std[std < 1e-6] = 1.0\n",
        "        X_sub = (X_sub - mean) / std\n",
        "    device = torch.device(device)\n",
        "    ds = TensorDataset(torch.from_numpy(X_sub).float().to(device), torch.from_numpy(y).float().to(device))\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "    if model_type == 'mlp':\n",
        "        model = SimpleMLP(X_sub.shape[1], hidden_dims=hidden_dims, dropout=dropout).to(device)\n",
        "    else:\n",
        "        model = SimpleLogistic(X_sub.shape[1]).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "    model.train()\n",
        "    for ep in range(epochs):\n",
        "        for xb, yb in loader:\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "    model.eval()\n",
        "    return {'model': model, 'mean': mean, 'std': std, 'device': device, 'standardize': standardize}\n",
        "\n",
        "\n",
        "def train_classifier(\n",
        "    Xin,\n",
        "    Xout,\n",
        "    selected_idxs,\n",
        "    responses,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    lr=1e-3,\n",
        "    device='cpu',\n",
        "    model_type='logistic',\n",
        "    hidden_dims=(64, 32),\n",
        "    dropout=0.2,\n",
        "    standardize=True\n",
        "):\n",
        "    X_feat, y = build_feature_matrix(selected_idxs, responses)\n",
        "    res = train_binary_model(\n",
        "        X_feat,\n",
        "        y,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        lr=lr,\n",
        "        device=device,\n",
        "        model_type=model_type,\n",
        "        hidden_dims=hidden_dims,\n",
        "        dropout=dropout,\n",
        "        standardize=standardize\n",
        "    )\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "baabea79",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_2d_scatter(X_feat, y, subset, kernel_idxs, out_path, boundary=None, title='Best 2-kernel feature space'):\n",
        "    if X_feat.size == 0 or y.size == 0:\n",
        "        return\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    cancer = y == 1\n",
        "    healthy = y == 0\n",
        "    ax.scatter(X_feat[cancer, subset[0]], X_feat[cancer, subset[1]], c='crimson', label='cancer', alpha=0.75, edgecolors='k', linewidths=0.3)\n",
        "    ax.scatter(X_feat[healthy, subset[0]], X_feat[healthy, subset[1]], c='teal', label='healthy', alpha=0.65, edgecolors='k', linewidths=0.3)\n",
        "\n",
        "    if boundary is not None and boundary.get('model') is not None:\n",
        "        model = boundary['model']\n",
        "        device = boundary.get('device', 'cpu')\n",
        "        mean = boundary.get('mean', None)\n",
        "        std = boundary.get('std', None)\n",
        "        standardize = boundary.get('standardize', False)\n",
        "        # build grid\n",
        "        x_min, x_max = X_feat[:, subset[0]].min(), X_feat[:, subset[0]].max()\n",
        "        y_min, y_max = X_feat[:, subset[1]].min(), X_feat[:, subset[1]].max()\n",
        "        pad_x = 0.1 * (x_max - x_min + 1e-6)\n",
        "        pad_y = 0.1 * (y_max - y_min + 1e-6)\n",
        "        xs = np.linspace(x_min - pad_x, x_max + pad_x, 200)\n",
        "        ys = np.linspace(y_min - pad_y, y_max + pad_y, 200)\n",
        "        xx, yy = np.meshgrid(xs, ys)\n",
        "        grid = np.stack([xx.ravel(), yy.ravel()], axis=1)\n",
        "        if standardize and mean is not None and std is not None:\n",
        "            grid = (grid - mean) / std\n",
        "        with torch.no_grad():\n",
        "            probs = torch.sigmoid(model(torch.from_numpy(grid).float().to(device))).cpu().numpy()\n",
        "        zz = probs.reshape(xx.shape)\n",
        "        ax.contourf(xx, yy, zz, levels=[0, 0.5, 1], alpha=0.18, colors=['teal', 'crimson'])\n",
        "        ax.contour(xx, yy, zz, levels=[0.5], colors='k', linewidths=1.0, linestyles='--')\n",
        "\n",
        "    ax.set_xlabel(f'Kernel {kernel_idxs[0]} response')\n",
        "    ax.set_ylabel(f'Kernel {kernel_idxs[1]} response')\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_path, dpi=200, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_3d_scatter(X_feat, y, subset, kernel_idxs, out_path):\n",
        "    if X_feat.size == 0 or y.size == 0:\n",
        "        return\n",
        "    fig = plt.figure(figsize=(7, 5))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    cancer = y == 1\n",
        "    healthy = y == 0\n",
        "    ax.scatter(X_feat[cancer, subset[0]], X_feat[cancer, subset[1]], X_feat[cancer, subset[2]], c='crimson', label='cancer', alpha=0.75, edgecolors='k', linewidths=0.3)\n",
        "    ax.scatter(X_feat[healthy, subset[0]], X_feat[healthy, subset[1]], X_feat[healthy, subset[2]], c='teal', label='healthy', alpha=0.65, edgecolors='k', linewidths=0.3)\n",
        "    ax.set_xlabel(f'Kernel {kernel_idxs[0]} response')\n",
        "    ax.set_ylabel(f'Kernel {kernel_idxs[1]} response')\n",
        "    ax.set_zlabel(f'Kernel {kernel_idxs[2]} response')\n",
        "    ax.set_title('Best 3-kernel feature space')\n",
        "    ax.legend()\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_path, dpi=220, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_roc_pr(y_true, probs, out_prefix):\n",
        "    if probs is None or y_true is None or len(y_true)==0:\n",
        "        return\n",
        "    fpr, tpr, _ = roc_curve(y_true, probs)\n",
        "    precision, recall, _ = precision_recall_curve(y_true, probs)\n",
        "    fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "    ax[0].plot(fpr, tpr, label=f\"AUC={roc_auc_score(y_true, probs):.3f}\")\n",
        "    ax[0].plot([0,1],[0,1],'k--',alpha=0.3)\n",
        "    ax[0].set_title('ROC'); ax[0].set_xlabel('FPR'); ax[0].set_ylabel('TPR'); ax[0].legend(); ax[0].grid(alpha=0.3)\n",
        "    ax[1].plot(recall, precision)\n",
        "    ax[1].set_title('Precision-Recall'); ax[1].set_xlabel('Recall'); ax[1].set_ylabel('Precision'); ax[1].grid(alpha=0.3)\n",
        "    fig.tight_layout(); fig.savefig(f\"{out_prefix}_roc_pr.png\", dpi=200, bbox_inches='tight'); plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_confusion(y_true, probs, out_path, threshold=0.5):\n",
        "    if probs is None or y_true is None or len(y_true)==0:\n",
        "        return\n",
        "    preds = (probs >= threshold).astype(int)\n",
        "    cm = confusion_matrix(y_true, preds)\n",
        "    fig, ax = plt.subplots(figsize=(4,4))\n",
        "    im = ax.imshow(cm, cmap='Blues')\n",
        "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
        "    ax.set_xticklabels(['healthy','cancer']); ax.set_yticklabels(['healthy','cancer'])\n",
        "    ax.set_ylabel('True'); ax.set_xlabel('Predicted')\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            ax.text(j, i, cm[i,j], ha='center', va='center', color='black')\n",
        "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    fig.tight_layout(); fig.savefig(out_path, dpi=200, bbox_inches='tight'); plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b118c2",
      "metadata": {},
      "source": [
        "---------------------- Configuration ----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "ca8704db",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-12T08:19:10.610408Z",
          "start_time": "2025-11-12T08:19:10.608339Z"
        }
      },
      "outputs": [],
      "source": [
        "IMAGE_DIR = \"data/TIFF Images/all\"       # folder with original mammograms\n",
        "ANNOT_DIR = \"data/Pixel-level annotation\"  # folder with annotated images (green contour)\n",
        "OUTPUT_DIR = \"data/patches\"          # output folder\n",
        "PATCH_SIZE = 128                # size of square patches\n",
        "N_INSIDE_PER_IMAGE = 50         # how many tumor patches per image\n",
        "N_OUTSIDE_PER_IMAGE = 100       # healthy patches per image\n",
        "GREEN_THRESH = 150              # threshold for green channel in contour\n",
        "MAX_TRIES = 500                 # max attempts to sample valid patch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44307b57",
      "metadata": {},
      "source": [
        "--------------------------- Main ---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "8fe9806b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-12T08:40:05.366808Z",
          "start_time": "2025-11-12T08:26:09.304275Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/511 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 192\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    166\u001b[0m     args \u001b[38;5;241m=\u001b[39m SimpleNamespace(\n\u001b[1;32m    167\u001b[0m         data_root\u001b[38;5;241m=\u001b[39mOUTPUT_DIR,\n\u001b[1;32m    168\u001b[0m         out_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m         standardize_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     )\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[36], line 27\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     25\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m annot \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask_from_green_contour\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m125\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m inside_patches, outside_patches \u001b[38;5;241m=\u001b[39m sample_patches(\n\u001b[1;32m     30\u001b[0m     img, mask, PATCH_SIZE, N_INSIDE_PER_IMAGE, N_OUTSIDE_PER_IMAGE, max_tries\u001b[38;5;241m=\u001b[39mMAX_TRIES \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# save patches\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[26], line 7\u001b[0m, in \u001b[0;36mmask_from_green_contour\u001b[0;34m(annot_img, GREEN_THRESH)\u001b[0m\n\u001b[1;32m      5\u001b[0m r,g,b \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39msplit(annot_img)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# mask = ((g < 140) & (r < 50) & ( > 190)).astype(np.uint8) #  Green\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Red\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# fill holes\u001b[39;00m\n\u001b[1;32m     10\u001b[0m mask \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdilate(mask, np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), np\u001b[38;5;241m.\u001b[39muint8), iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "annot = None\n",
        "def main(args):\n",
        "    device = 'cuda' if torch.cuda.is_available() and not args.force_cpu else 'cpu'\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "\n",
        "    # Make the patches directory\n",
        "    output_dir = Path(OUTPUT_DIR)\n",
        "    (output_dir / \"inside\").mkdir(parents=True, exist_ok=True)\n",
        "    (output_dir / \"outside\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    image_paths = sorted(list(Path(IMAGE_DIR).glob(\"*.tif\")))\n",
        "    annot_paths = sorted(list(Path(IMAGE_DIR).glob(\"*.tif\")))\n",
        "\n",
        "    inside_idx = 0\n",
        "    outside_idx = 0\n",
        "\n",
        "    # for img_path, annot_path in tqdm(zip(image_paths, annot_paths), total=len(image_paths)):\n",
        "    for img_path in tqdm(image_paths, total=len(image_paths)):\n",
        "        img = cv2.imread(str(img_path), cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "        annot = None\n",
        "        if os.path.exists(str(str(Path(ANNOT_DIR) / Path(img_path.stem + '.tif')))):\n",
        "            annot = cv2.imread(str(Path(ANNOT_DIR) / Path(img_path.stem + '.tif')), cv2.IMREAD_COLOR_RGB)\n",
        "\n",
        "        mask = None\n",
        "        if annot is not None:\n",
        "            mask = mask_from_green_contour(annot, 125)\n",
        "\n",
        "        inside_patches, outside_patches = sample_patches(\n",
        "            img, mask, PATCH_SIZE, N_INSIDE_PER_IMAGE, N_OUTSIDE_PER_IMAGE, max_tries=MAX_TRIES * 20\n",
        "        )\n",
        "\n",
        "        # save patches\n",
        "        for p in inside_patches:\n",
        "            pil = Image.fromarray(p)\n",
        "            pil.save(output_dir / \"inside\" / f\"inside_{inside_idx:05d}.tif\")\n",
        "            inside_idx += 1\n",
        "\n",
        "        for p in outside_patches:\n",
        "            pil = Image.fromarray(p)\n",
        "            pil.save(output_dir / \"outside\" / f\"outside_{outside_idx:05d}.tif\")\n",
        "            outside_idx += 1\n",
        "\n",
        "    print(f\"Saved {inside_idx} inside patches and {outside_idx} outside patches to {OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "    # load data\n",
        "    Xin, Xout = load_patches_from_folders(args.data_root, max_per_class=args.max_per_class, resize=(args.patch_size,args.patch_size))\n",
        "\n",
        "    print(f'Loaded: in={len(Xin)} out={len(Xout)} patches. Device={device}')\n",
        "    # build kernel bank\n",
        "    families = args.families.split(',')\n",
        "    bank = build_kernel_bank(families, args.n_per_family, args.kernel_size)\n",
        "    print(f'Built kernel bank: {len(bank)} kernels')\n",
        "\n",
        "    # compute responses\n",
        "    responses = compute_responses(bank, Xin, Xout, device=device, batch_size=args.batch_size, response_fn=args.response_fn)\n",
        "\n",
        "    # select diverse top kernels\n",
        "    selected_idxs = select_diverse(bank, responses, topM=args.topM, K=args.K, lambda_mm=args.lambda_mm)\n",
        "    print('Selected kernel indices:', selected_idxs)\n",
        "\n",
        "    # train classifier\n",
        "    clf_res = train_classifier(\n",
        "        Xin,\n",
        "        Xout,\n",
        "        selected_idxs,\n",
        "        responses,\n",
        "        epochs=args.epochs,\n",
        "        batch_size=args.batch_size,\n",
        "        lr=args.lr,\n",
        "        device=device,\n",
        "        model_type=args.model_type,\n",
        "        hidden_dims=(args.hidden_dim1, args.hidden_dim2),\n",
        "        dropout=args.dropout,\n",
        "        standardize=args.standardize_features\n",
        "    )\n",
        "    print(f\"Classifier val AUC={clf_res['auc']:.4f} ACC={clf_res['acc']:.4f}\")\n",
        "\n",
        "    # Performance plots\n",
        "    plot_roc_pr(clf_res.get('val_labels'), clf_res.get('val_probs'), str(Path(args.out_dir) / 'clf'))\n",
        "    plot_confusion(clf_res.get('val_labels'), clf_res.get('val_probs'), str(Path(args.out_dir) / 'confusion.png'))\n",
        "\n",
        "    # Visualize best 2- and 3-kernel subsets\n",
        "    kernel_scores = rank_kernels(responses)\n",
        "    candidate_kernel_idxs = [s['idx'] for s in kernel_scores[:args.plot_top_kernels]]\n",
        "    X_candidates, y_labels = build_feature_matrix(candidate_kernel_idxs, responses)\n",
        "    subset_results = find_best_subsets(\n",
        "        X_candidates,\n",
        "        y_labels,\n",
        "        subset_sizes=[2,3],\n",
        "        epochs=args.subset_search_epochs,\n",
        "        batch_size=args.batch_size,\n",
        "        lr=args.lr,\n",
        "        device=device,\n",
        "        model_type=args.model_type,\n",
        "        hidden_dims=(args.hidden_dim1, args.hidden_dim2),\n",
        "        dropout=args.dropout,\n",
        "        standardize=args.standardize_features\n",
        "    )\n",
        "\n",
        "    pair_plot = Path(args.out_dir) / 'scatter_best_pair.png'\n",
        "    triple_plot = Path(args.out_dir) / 'scatter_best_triple.png'\n",
        "    if 2 in subset_results:\n",
        "        pair = subset_results[2]\n",
        "        pair_kernel_idxs = [candidate_kernel_idxs[i] for i in pair['subset']]\n",
        "        boundary_model = fit_subset_model(\n",
        "            X_candidates,\n",
        "            y_labels,\n",
        "            pair['subset'],\n",
        "            epochs=args.boundary_epochs,\n",
        "            batch_size=args.batch_size,\n",
        "            lr=args.lr,\n",
        "            device=device,\n",
        "            model_type=args.model_type,\n",
        "            hidden_dims=(args.hidden_dim1, args.hidden_dim2),\n",
        "            dropout=args.dropout,\n",
        "            standardize=args.standardize_features\n",
        "        )\n",
        "        plot_2d_scatter(\n",
        "            X_candidates,\n",
        "            y_labels,\n",
        "            pair['subset'],\n",
        "            pair_kernel_idxs,\n",
        "            pair_plot,\n",
        "            boundary=boundary_model,\n",
        "            title='Best 2-kernel feature space with decision boundary'\n",
        "        )\n",
        "        print(f\"Best 2-kernel subset {pair_kernel_idxs} AUC={pair['auc']:.4f} saved to {pair_plot}\")\n",
        "    else:\n",
        "        print('Not enough kernels to make 2D scatter plot')\n",
        "\n",
        "    if 3 in subset_results:\n",
        "        triple = subset_results[3]\n",
        "        triple_kernel_idxs = [candidate_kernel_idxs[i] for i in triple['subset']]\n",
        "        plot_3d_scatter(X_candidates, y_labels, triple['subset'], triple_kernel_idxs, triple_plot)\n",
        "        print(f\"Best 3-kernel subset {triple_kernel_idxs} AUC={triple['auc']:.4f} saved to {triple_plot}\")\n",
        "    else:\n",
        "        print('Not enough kernels to make 3D scatter plot')\n",
        "\n",
        "    # save results\n",
        "    out = {\n",
        "        'selected_idxs': selected_idxs,\n",
        "        'bank_meta': [{'family': b['family'], 'params': b['params']} for b in bank],\n",
        "        'clf_auc': float(clf_res['auc']),\n",
        "        'clf_acc': float(clf_res['acc'])\n",
        "    }\n",
        "    # Save for fast reuse\n",
        "    np.savez(\n",
        "        'results/feature_cache.npz',\n",
        "        X_candidates=X_candidates,\n",
        "        y_labels=y_labels,\n",
        "        subset_results=subset_results,\n",
        "        candidate_kernel_idxs=np.array(candidate_kernel_idxs)\n",
        "    )\n",
        "    np.savez_compressed(os.path.join(args.out_dir, 'results.npz'), selected_idxs=np.array(selected_idxs))\n",
        "    with open(os.path.join(args.out_dir, 'results.json'), 'w') as f:\n",
        "        json.dump(out, f, indent=2)\n",
        "    # save kernels\n",
        "    kernels = np.stack([b['kernel'] for b in bank], axis=0)\n",
        "    np.save(os.path.join(args.out_dir, 'kernels.npy'), kernels)\n",
        "    print('Saved results to', args.out_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = SimpleNamespace(\n",
        "        data_root=OUTPUT_DIR,\n",
        "        out_dir='./results',\n",
        "        families='gaussian,anisotropic_gaussian,dog,log,gabor',\n",
        "        n_per_family=200,\n",
        "        kernel_size=31,\n",
        "        patch_size=64,\n",
        "        max_per_class=2000,\n",
        "        batch_size=64,\n",
        "        response_fn='mean_abs',\n",
        "        topM=200,\n",
        "        K=20,\n",
        "        lambda_mm=0.75,\n",
        "        epochs=60,\n",
        "        lr=5e-4,\n",
        "        force_cpu=False,\n",
        "        plot_top_kernels=20,\n",
        "        subset_search_epochs=20,\n",
        "        boundary_epochs=25,\n",
        "        model_type='mlp',\n",
        "        hidden_dim1=64,\n",
        "        hidden_dim2=32,\n",
        "        dropout=0.2,\n",
        "        standardize_features=True\n",
        "    )\n",
        "\n",
        "    main(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fb66437",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cache = np.load('results/feature_cache.npz', allow_pickle=True)\n",
        "X_candidates = cache['X_candidates']\n",
        "y_labels = cache['y_labels']\n",
        "subset_results = cache['subset_results'].item()\n",
        "candidate_kernel_idxs = cache['candidate_kernel_idxs']\n",
        "\n",
        "# 2D boundary plot (re-fit quickly on the saved pair)\n",
        "pair = subset_results[2]\n",
        "pair_kernel_idxs = [candidate_kernel_idxs[i] for i in pair['subset']]\n",
        "boundary_model = fit_subset_model(\n",
        "    X_candidates, y_labels, pair['subset'],\n",
        "    epochs=20, batch_size=64, lr=5e-4,\n",
        "    device='cpu', model_type='mlp',\n",
        "    hidden_dims=(64, 32), dropout=0.2,\n",
        "    standardize=True\n",
        ")\n",
        "plot_2d_scatter(\n",
        "    X_candidates, y_labels, pair['subset'], pair_kernel_idxs,\n",
        "    'results/scatter_best_pair_cached.png', boundary=boundary_model,\n",
        "    title='Best 2-kernel feature space (cached)'\n",
        ")\n",
        "\n",
        "# Per-feature histograms\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
        "for cls, color in [(1, 'crimson'), (0, 'teal')]:\n",
        "    ax[0].hist(X_candidates[y_labels==cls, pair['subset'][0]], bins=30, alpha=0.6, color=color, label=f'class {cls}')\n",
        "    ax[1].hist(X_candidates[y_labels==cls, pair['subset'][1]], bins=30, alpha=0.6, color=color, label=f'class {cls}')\n",
        "ax[0].set_title('Kernel ' + str(pair_kernel_idxs[0])); ax[1].set_title('Kernel ' + str(pair_kernel_idxs[1]))\n",
        "for a in ax: a.legend(); a.grid(alpha=0.3)\n",
        "fig.tight_layout(); plt.savefig('results/hist_pair_cached.png', dpi=200); plt.close()\n",
        "\n",
        "# ROC on the saved pair\n",
        "from sklearn.metrics import roc_curve\n",
        "model = boundary_model['model']; mean = boundary_model['mean']; std = boundary_model['std']; standardize = boundary_model['standardize']\n",
        "X_pair = X_candidates[:, pair['subset']]\n",
        "if standardize:\n",
        "    X_pair = (X_pair - mean) / std\n",
        "probs = torch.sigmoid(model(torch.from_numpy(X_pair).float())).detach().cpu().numpy()\n",
        "fpr, tpr, _ = roc_curve(y_labels, probs)\n",
        "plt.plot(fpr, tpr); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC (cached pair)')\n",
        "plt.savefig('results/roc_pair_cached.png', dpi=200); plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1c87dc5",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "all_in_one",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
