{
 "cells": [
  {
   "cell_type": "code",
   "id": "b2e6273d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T08:19:10.189500Z",
     "start_time": "2025-11-12T08:19:08.227573Z"
    }
   },
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.ndimage import binary_fill_holes\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "56d7e08e",
   "metadata": {},
   "source": [
    "--------------------------- Utilities ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed5117fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T08:19:10.206231Z",
     "start_time": "2025-11-12T08:19:10.195133Z"
    }
   },
   "source": [
    "def mask_from_green_contour(annot_img, GREEN_THRESH):\n",
    "    \"\"\"\n",
    "    Returns a binary mask where tumor area (green contour) = 1\n",
    "    \"\"\"\n",
    "    r,g,b = cv2.split(annot_img)\n",
    "    # mask = ((g < 140) & (r < 50) & ( > 190)).astype(np.uint8) #  Green\n",
    "    mask = ((g < 70) & (b < 70) & (r > 150)).astype(np.uint8)  # Red\n",
    "\n",
    "    # fill holes\n",
    "    mask = cv2.dilate(mask, np.ones((3,3), np.uint8), iterations=2)\n",
    "    mask = cv2.erode(mask, np.ones((3,3), np.uint8), iterations=2)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
    "    mask = binary_fill_holes(mask).astype(np.uint8)\n",
    "    mask = cv2.cvtColor(mask*255, cv2.COLOR_GRAY2BGR)[:,:,0] > 0\n",
    "\n",
    "    return mask\n",
    "\n",
    "def sample_patches(img, mask, patch_size, n_inside, n_outside, max_tries=500):\n",
    "    H,W = img.shape[:2]\n",
    "    inside_patches, outside_patches = [], []\n",
    "\n",
    "    # inside\n",
    "    if mask is not None:\n",
    "        tries = 0\n",
    "        while len(inside_patches) < n_inside and tries < max_tries:\n",
    "            x = random.randint(0, W - patch_size)\n",
    "            y = random.randint(0, H - patch_size)\n",
    "            patch_mask = mask[y:y+patch_size, x:x+patch_size]\n",
    "            # require at least 80% inside mask\n",
    "            if patch_mask.mean() > 0.8:\n",
    "                patch = img[y:y+patch_size, x:x+patch_size]\n",
    "                inside_patches.append(patch)\n",
    "            tries += 1\n",
    "    else:\n",
    "        mask = np.zeros((*img.shape[:2], 3), dtype=np.uint8)\n",
    "\n",
    "    # outside\n",
    "    if img is not None:\n",
    "        tries = 0\n",
    "        while len(outside_patches) < n_outside and tries < max_tries:\n",
    "            x = random.randint(0, W - patch_size)\n",
    "            y = random.randint(0, H - patch_size)\n",
    "            patch_mask = mask[y:y+patch_size, x:x+patch_size]\n",
    "            # require <5% overlap with tumor\n",
    "            if patch_mask.mean() < 0.05 and patch_mask.mean() != 0:\n",
    "                patch = img[y:y+patch_size, x:x+patch_size]\n",
    "                outside_patches.append(patch)\n",
    "            tries += 1\n",
    "\n",
    "    return inside_patches, outside_patches\n",
    "\n",
    "\n",
    "def load_patches_from_folders(root_dir, max_per_class=None, gray=True, resize=None):\n",
    "    \"\"\"Expect two folders inside root_dir: 'inside' and 'outside'.\"\"\"\n",
    "    inside_dir = Path(root_dir) / 'inside'\n",
    "    outside_dir = Path(root_dir) / 'outside'\n",
    "    os.makedirs(inside_dir, exist_ok=True)\n",
    "    os.makedirs(outside_dir, exist_ok=True)\n",
    "\n",
    "    assert inside_dir.exists() and outside_dir.exists(), \"Folders 'inside' and 'outside' required\"\n",
    "\n",
    "    def load_from(folder, limit):\n",
    "        files = sorted([p for p in folder.iterdir() if p.suffix.lower() in ('.tif','.jpg','.jpeg','.tif','.tiff')])\n",
    "        if limit:\n",
    "            files = files[:limit]\n",
    "        imgs = []\n",
    "        for p in files:\n",
    "            im = Image.open(p)\n",
    "            if gray:\n",
    "                im = im.convert('L')\n",
    "            else:\n",
    "                im = im.convert('RGB')\n",
    "            if resize:\n",
    "                im = im.resize(resize, Image.BILINEAR)\n",
    "            arr = np.array(im, dtype=np.float32) / 255.0\n",
    "            imgs.append(arr)\n",
    "        return np.stack(imgs, axis=0) if imgs else np.zeros((0, *(resize if resize else im.size[::-1])), dtype=np.float32)\n",
    "\n",
    "    Xin = load_from(inside_dir, max_per_class)\n",
    "    Xout = load_from(outside_dir, max_per_class)\n",
    "    # ensure shape N,H,W\n",
    "    return Xin, Xout\n",
    "\n",
    "def to_tensor_batch(X, device):\n",
    "    # X: N,H,W -> return tensor N,1,H,W\n",
    "    t = torch.from_numpy(X).unsqueeze(1).to(device)\n",
    "    return t"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "a9a043b6",
   "metadata": {},
   "source": [
    "--------------------------- Kernel generators ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "id": "a97d4838",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T08:19:10.253702Z",
     "start_time": "2025-11-12T08:19:10.248557Z"
    }
   },
   "source": [
    "def make_gaussian_kernel(size, sigma_x, sigma_y=None, theta=0.0):\n",
    "    if sigma_y is None:\n",
    "        sigma_y = sigma_x\n",
    "    assert size % 2 == 1, 'size should be odd'\n",
    "    half = size // 2\n",
    "    xs = np.arange(-half, half+1, 1)\n",
    "    ys = np.arange(-half, half+1, 1)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    # rotate\n",
    "    ct = math.cos(theta); st = math.sin(theta)\n",
    "    Xr = ct * X + st * Y\n",
    "    Yr = -st * X + ct * Y\n",
    "    G = np.exp(-0.5 * ((Xr**2) / (sigma_x**2 + 1e-12) + (Yr**2) / (sigma_y**2 + 1e-12)))\n",
    "    G = G / (G.sum() + 1e-12)\n",
    "    return G.astype(np.float32)\n",
    "\n",
    "\n",
    "def make_dog_kernel(size, sigma1, sigma2, theta=0.0):\n",
    "    g1 = make_gaussian_kernel(size, sigma1, sigma1, theta)\n",
    "    g2 = make_gaussian_kernel(size, sigma2, sigma2, theta)\n",
    "    k = g1 - g2\n",
    "    # normalize zero mean\n",
    "    k = k - k.mean()\n",
    "    return k.astype(np.float32)\n",
    "\n",
    "\n",
    "def make_log_kernel(size, sigma, theta=0.0):\n",
    "    # Laplacian of Gaussian approximation\n",
    "    assert size % 2 == 1\n",
    "    half = size // 2\n",
    "    xs = np.arange(-half, half+1, 1)\n",
    "    ys = np.arange(-half, half+1, 1)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    ct = math.cos(theta); st = math.sin(theta)\n",
    "    Xr = ct * X + st * Y\n",
    "    Yr = -st * X + ct * Y\n",
    "    r2 = (Xr**2 + Yr**2)\n",
    "    s2 = sigma**2\n",
    "    LoG = ((r2 - 2*s2) / (s2**2)) * np.exp(-r2/(2*s2))\n",
    "    LoG = LoG - LoG.mean()\n",
    "    return LoG.astype(np.float32)\n",
    "\n",
    "\n",
    "def make_gabor_kernel(size, sigma, freq, theta=0.0, phase=0.0, gamma=1.0):\n",
    "    # gamma: aspect ratio\n",
    "    assert size % 2 == 1\n",
    "    half = size // 2\n",
    "    xs = np.arange(-half, half+1, 1)\n",
    "    ys = np.arange(-half, half+1, 1)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    ct = math.cos(theta); st = math.sin(theta)\n",
    "    Xr = ct * X + st * Y\n",
    "    Yr = -st * X + ct * Y\n",
    "    Yr = Yr * gamma\n",
    "    gaussian = np.exp(-(Xr**2 + Yr**2) / (2 * (sigma**2)))\n",
    "    sinusoid = np.cos(2 * np.pi * freq * Xr + phase)\n",
    "    K = gaussian * sinusoid\n",
    "    # zero mean\n",
    "    K = K - K.mean()\n",
    "    # normalize by L1\n",
    "    if K.sum() != 0:\n",
    "        K = K / (np.abs(K).sum() + 1e-12)\n",
    "    return K.astype(np.float32)\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "be043854",
   "metadata": {},
   "source": [
    "--------------------------- Sampling parameters ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "id": "4e063afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T08:19:10.304732Z",
     "start_time": "2025-11-12T08:19:10.300694Z"
    }
   },
   "source": [
    "def sample_parameters(family, n_samples, size):\n",
    "    params = []\n",
    "    for _ in range(n_samples):\n",
    "        if family == 'gaussian':\n",
    "            sigma = float(10 ** np.random.uniform(np.log10(0.5), np.log10(size/2)))\n",
    "            theta = np.random.uniform(0, math.pi)\n",
    "            params.append({'sigma_x': sigma, 'sigma_y': sigma, 'theta': theta, 'size': size})\n",
    "        elif family == 'anisotropic_gaussian':\n",
    "            sigma_x = float(10 ** np.random.uniform(np.log10(0.5), np.log10(size/2)))\n",
    "            sigma_y = float(sigma_x * np.random.uniform(0.5, 3.0))\n",
    "            theta = np.random.uniform(0, math.pi)\n",
    "            params.append({'sigma_x': sigma_x, 'sigma_y': sigma_y, 'theta': theta, 'size': size})\n",
    "        elif family == 'dog':\n",
    "            s1 = float(np.random.uniform(0.5, size/2))\n",
    "            s2 = float(s1 * np.random.uniform(1.2, 3.0))\n",
    "            theta = np.random.uniform(0, math.pi)\n",
    "            params.append({'sigma1': s1, 'sigma2': s2, 'theta': theta, 'size': size})\n",
    "        elif family == 'log':\n",
    "            s = float(np.random.uniform(0.5, size/2))\n",
    "            theta = np.random.uniform(0, math.pi)\n",
    "            params.append({'sigma': s, 'theta': theta, 'size': size})\n",
    "        elif family == 'gabor':\n",
    "            sigma = float(np.random.uniform(0.5, size/2))\n",
    "            freq = float(np.random.uniform(0.02, 0.5))\n",
    "            theta = np.random.uniform(0, math.pi)\n",
    "            phase = float(np.random.uniform(0, 2*math.pi))\n",
    "            gamma = float(np.random.uniform(0.5, 1.5))\n",
    "            params.append({'sigma': sigma, 'freq': freq, 'theta': theta, 'phase': phase, 'gamma': gamma, 'size': size})\n",
    "        else:\n",
    "            raise ValueError('Unknown family')\n",
    "    return params\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "17c12c3e",
   "metadata": {},
   "source": [
    "--------------------------- Kernel bank builder ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "id": "93136a57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T08:19:10.354440Z",
     "start_time": "2025-11-12T08:19:10.351571Z"
    }
   },
   "source": [
    "\n",
    "def build_kernel_bank(families, n_per_family, size):\n",
    "    bank = []\n",
    "    for fam in families:\n",
    "        params = sample_parameters(fam, n_per_family, size)\n",
    "        for p in params:\n",
    "            if fam == 'gaussian' or fam == 'anisotropic_gaussian':\n",
    "                k = make_gaussian_kernel(p['size'], p['sigma_x'], p.get('sigma_y', None), p['theta'])\n",
    "            elif fam == 'dog':\n",
    "                k = make_dog_kernel(p['size'], p['sigma1'], p['sigma2'], p['theta'])\n",
    "            elif fam == 'log':\n",
    "                k = make_log_kernel(p['size'], p['sigma'], p['theta'])\n",
    "            elif fam == 'gabor':\n",
    "                k = make_gabor_kernel(p['size'], p['sigma'], p['freq'], p['theta'], p['phase'], p['gamma'])\n",
    "            else:\n",
    "                continue\n",
    "            bank.append({'family': fam, 'params': p, 'kernel': k})\n",
    "    return bank"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "a5494e3e",
   "metadata": {},
   "source": [
    "--------------------------- Response computation ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "id": "c68726a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T08:19:10.408475Z",
     "start_time": "2025-11-12T08:19:10.404194Z"
    }
   },
   "source": [
    "def compute_responses(bank, X_in, X_out, device='cpu', batch_size=64, response_fn='abs_max'):\n",
    "    device = torch.device(device)\n",
    "    Xin_t = to_tensor_batch(X_in, device)\n",
    "    Xout_t = to_tensor_batch(X_out, device)\n",
    "    responses = []\n",
    "    # convert bank kernels to torch tensors (filters)\n",
    "    filter_tensors = []\n",
    "    for entry in bank:\n",
    "        k = entry['kernel']\n",
    "        k_t = torch.from_numpy(k).unsqueeze(0).unsqueeze(0).to(device)  # 1,1,Hk,Wk\n",
    "        filter_tensors.append(k_t)\n",
    "    # compute responses per kernel\n",
    "    for idx, k_t in enumerate(tqdm(filter_tensors, desc='Kernels')):\n",
    "        # conv Xin\n",
    "        r_in = []\n",
    "        for i in range(0, Xin_t.shape[0], batch_size):\n",
    "            batch = Xin_t[i:i+batch_size]\n",
    "            with torch.no_grad():\n",
    "                out = F.conv2d(batch, k_t, padding=k_t.shape[-1]//2)\n",
    "                if response_fn == 'abs_max':\n",
    "                    val = out.abs().amax(dim=[1,2,3]).cpu().numpy()\n",
    "                elif response_fn == 'mean_abs':\n",
    "                    val = out.abs().mean(dim=[1,2,3]).cpu().numpy()\n",
    "                else:\n",
    "                    val = out.abs().amax(dim=[1,2,3]).cpu().numpy()\n",
    "                r_in.append(val)\n",
    "        r_in = np.concatenate(r_in, axis=0) if len(r_in) else np.zeros((0,))\n",
    "\n",
    "        r_out = []\n",
    "        for i in range(0, Xout_t.shape[0], batch_size):\n",
    "            batch = Xout_t[i:i+batch_size]\n",
    "            with torch.no_grad():\n",
    "                out = F.conv2d(batch, k_t, padding=k_t.shape[-1]//2)\n",
    "                if response_fn == 'abs_max':\n",
    "                    val = out.abs().amax(dim=[1,2,3]).cpu().numpy()\n",
    "                elif response_fn == 'mean_abs':\n",
    "                    val = out.abs().mean(dim=[1,2,3]).cpu().numpy()\n",
    "                else:\n",
    "                    val = out.abs().amax(dim=[1,2,3]).cpu().numpy()\n",
    "                r_out.append(val)\n",
    "        r_out = np.concatenate(r_out, axis=0) if len(r_out) else np.zeros((0,))\n",
    "\n",
    "        responses.append({'r_in': r_in, 'r_out': r_out})\n",
    "    return responses\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "75f09f9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T08:19:10.454629Z",
     "start_time": "2025-11-12T08:19:10.451494Z"
    }
   },
   "source": [
    "\n",
    "def fisher_score(r_in, r_out):\n",
    "    mu_in = r_in.mean() if r_in.size else 0.0\n",
    "    mu_out = r_out.mean() if r_out.size else 0.0\n",
    "    var_in = r_in.var(ddof=1) if r_in.size>1 else 0.0\n",
    "    var_out = r_out.var(ddof=1) if r_out.size>1 else 0.0\n",
    "    num = (mu_in - mu_out)**2\n",
    "    den = var_in + var_out + 1e-12\n",
    "    return float(num / den)\n",
    "\n",
    "\n",
    "def auc_score(r_in, r_out):\n",
    "    y_true = np.concatenate([np.ones(len(r_in)), np.zeros(len(r_out))])\n",
    "    y_score = np.concatenate([r_in, r_out])\n",
    "    # if constant, AUC undefined -> return 0.5\n",
    "    try:\n",
    "        if y_score.max() == y_score.min():\n",
    "            return 0.5\n",
    "        return float(roc_auc_score(y_true, y_score))\n",
    "    except Exception:\n",
    "        return 0.5\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "2eca35d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T08:19:10.508286Z",
     "start_time": "2025-11-12T08:19:10.503694Z"
    }
   },
   "source": [
    "def pairwise_response_corr(resp_i, resp_j):\n",
    "    # responses already 1D arrays on combined patches\n",
    "    a = resp_i - resp_i.mean()\n",
    "    b = resp_j - resp_j.mean()\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b) + 1e-12)\n",
    "    return float(np.dot(a,b) / denom)\n",
    "\n",
    "\n",
    "def select_diverse(bank, responses, topM=200, K=20, lambda_mm=0.75):\n",
    "    # rank by combined score (we'll use AUC primarily then fisher)\n",
    "    scores = []\n",
    "    for i, resp in enumerate(responses):\n",
    "        auc = auc_score(resp['r_in'], resp['r_out'])\n",
    "        fisher = fisher_score(resp['r_in'], resp['r_out'])\n",
    "        scores.append({'idx': i, 'auc': auc, 'fisher': fisher})\n",
    "    scores = sorted(scores, key=lambda x: (x['auc'], x['fisher']), reverse=True)\n",
    "    top_idxs = [s['idx'] for s in scores[:topM]]\n",
    "    # precompute combined response vectors\n",
    "    combined = []\n",
    "    for i in top_idxs:\n",
    "        combined.append(np.concatenate([responses[i]['r_in'], responses[i]['r_out']]))\n",
    "    selected = []\n",
    "    selected_idxs = []\n",
    "    # greedy MMR\n",
    "    for rank_i, idx in enumerate(top_idxs):\n",
    "        if not selected:\n",
    "            selected.append(idx)\n",
    "            selected_idxs.append(idx)\n",
    "            if len(selected) >= K:\n",
    "                break\n",
    "            continue\n",
    "        # compute mmr score for remaining\n",
    "        best_score = -1e9\n",
    "        best_idx = None\n",
    "        for j, cand in enumerate(top_idxs):\n",
    "            if cand in selected_idxs:\n",
    "                continue\n",
    "            auc_j = next(s for s in scores if s['idx']==cand)['auc']\n",
    "            # similarity to selected (max)\n",
    "            sims = [abs(pairwise_response_corr(combined[top_idxs.index(cand)], combined[top_idxs.index(s)])) for s in selected]\n",
    "            maxsim = max(sims) if sims else 0.0\n",
    "            mmr = lambda_mm * auc_j - (1-lambda_mm) * maxsim\n",
    "            if mmr > best_score:\n",
    "                best_score = mmr\n",
    "                best_idx = cand\n",
    "        if best_idx is None:\n",
    "            break\n",
    "        selected.append(best_idx)\n",
    "        selected_idxs.append(best_idx)\n",
    "        if len(selected) >= K:\n",
    "            break\n",
    "    return selected_idxs\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "0bbb23bc",
   "metadata": {},
   "source": [
    "--------------------------- Training classifier ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "id": "0eeb730f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T08:19:10.561535Z",
     "start_time": "2025-11-12T08:19:10.555661Z"
    }
   },
   "source": [
    "class SimpleLogistic(torch.nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(n_features, 1)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x).squeeze(1)\n",
    "\n",
    "\n",
    "def train_classifier(Xin, Xout, selected_idxs, responses, epochs=30, batch_size=64, lr=1e-3, device='cpu'):\n",
    "    # build feature matrix\n",
    "    X_all = np.concatenate([Xin, Xout], axis=0)\n",
    "    y = np.concatenate([np.ones(len(Xin)), np.zeros(len(Xout))])\n",
    "    features = []\n",
    "    for idx in selected_idxs:\n",
    "        r = np.concatenate([responses[idx]['r_in'], responses[idx]['r_out']])\n",
    "        features.append(r.reshape(-1,1))\n",
    "    X_feat = np.concatenate(features, axis=1)\n",
    "    # train/test split\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_feat, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    device = torch.device(device)\n",
    "    tr_ds = TensorDataset(torch.from_numpy(X_tr).float().to(device), torch.from_numpy(y_tr).float().to(device))\n",
    "    val_ds = TensorDataset(torch.from_numpy(X_val).float().to(device), torch.from_numpy(y_val).float().to(device))\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = SimpleLogistic(X_feat.shape[1]).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_auc = 0.0\n",
    "    best_state = None\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in tr_loader:\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "        # validate\n",
    "        model.eval()\n",
    "        ys = []\n",
    "        ps = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                logits = model(xb)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                ys.append(yb.cpu().numpy())\n",
    "                ps.append(probs)\n",
    "        ys = np.concatenate(ys)\n",
    "        ps = np.concatenate(ps)\n",
    "        try:\n",
    "            auc = roc_auc_score(ys, ps)\n",
    "        except Exception:\n",
    "            auc = 0.5\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    # final metrics on validation\n",
    "    model.eval()\n",
    "    X_val_t = torch.from_numpy(X_val).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(model(X_val_t)).cpu().numpy()\n",
    "    acc = accuracy_score(y_val, (probs>0.5).astype(int))\n",
    "    auc = roc_auc_score(y_val, probs)\n",
    "    return {'model': model, 'auc': auc, 'acc': acc}\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "e8b118c2",
   "metadata": {},
   "source": [
    "---------------------- Configuration ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "id": "ca8704db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T08:19:10.610408Z",
     "start_time": "2025-11-12T08:19:10.608339Z"
    }
   },
   "source": [
    "IMAGE_DIR = \"data/TIFF Images/all\"       # folder with original mammograms\n",
    "ANNOT_DIR = \"data/Pixel-level annotation\"  # folder with annotated images (green contour)\n",
    "OUTPUT_DIR = \"data/patches\"          # output folder\n",
    "PATCH_SIZE = 128                # size of square patches\n",
    "N_INSIDE_PER_IMAGE = 50         # how many tumor patches per image\n",
    "N_OUTSIDE_PER_IMAGE = 100       # healthy patches per image\n",
    "GREEN_THRESH = 150              # threshold for green channel in contour\n",
    "MAX_TRIES = 500                 # max attempts to sample valid patch\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "44307b57",
   "metadata": {},
   "source": [
    "--------------------------- Main ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "id": "8fe9806b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T08:40:05.366808Z",
     "start_time": "2025-11-12T08:26:09.304275Z"
    }
   },
   "source": [
    "annot = None\n",
    "def main(args):\n",
    "    device = 'cuda' if torch.cuda.is_available() and not args.force_cpu else 'cpu'\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "    # Make the patches directory\n",
    "    output_dir = Path(OUTPUT_DIR)\n",
    "    (output_dir / \"inside\").mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / \"outside\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    image_paths = sorted(list(Path(IMAGE_DIR).glob(\"*.tif\")))\n",
    "    annot_paths = sorted(list(Path(IMAGE_DIR).glob(\"*.tif\")))\n",
    "\n",
    "    inside_idx = 0\n",
    "    outside_idx = 0\n",
    "\n",
    "    # for img_path, annot_path in tqdm(zip(image_paths, annot_paths), total=len(image_paths)):\n",
    "    for img_path in tqdm(image_paths, total=len(image_paths)):\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        annot = None\n",
    "        if os.path.exists(str(str(Path(ANNOT_DIR) / Path(img_path.stem + '.tif')))):\n",
    "            annot = cv2.imread(str(Path(ANNOT_DIR) / Path(img_path.stem + '.tif')), cv2.IMREAD_COLOR_RGB)\n",
    "\n",
    "        mask = None\n",
    "        if annot is not None:\n",
    "            mask = mask_from_green_contour(annot, 125)\n",
    "\n",
    "        inside_patches, outside_patches = sample_patches(\n",
    "            img, mask, PATCH_SIZE, N_INSIDE_PER_IMAGE, N_OUTSIDE_PER_IMAGE, max_tries=MAX_TRIES * 20\n",
    "        )\n",
    "\n",
    "        # save patches\n",
    "        for p in inside_patches:\n",
    "            pil = Image.fromarray(p)\n",
    "            pil.save(output_dir / \"inside\" / f\"inside_{inside_idx:05d}.tif\")\n",
    "            inside_idx += 1\n",
    "\n",
    "        for p in outside_patches:\n",
    "            pil = Image.fromarray(p)\n",
    "            pil.save(output_dir / \"outside\" / f\"outside_{outside_idx:05d}.tif\")\n",
    "            outside_idx += 1\n",
    "\n",
    "    print(f\"Saved {inside_idx} inside patches and {outside_idx} outside patches to {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "    # load data\n",
    "    Xin, Xout = load_patches_from_folders(args.data_root, max_per_class=args.max_per_class, resize=(args.patch_size,args.patch_size))\n",
    "\n",
    "    print(f'Loaded: in={len(Xin)} out={len(Xout)} patches. Device={device}')\n",
    "    # build kernel bank\n",
    "    families = args.families.split(',')\n",
    "    bank = build_kernel_bank(families, args.n_per_family, args.kernel_size)\n",
    "    print(f'Built kernel bank: {len(bank)} kernels')\n",
    "\n",
    "    # compute responses\n",
    "    responses = compute_responses(bank, Xin, Xout, device=device, batch_size=args.batch_size, response_fn=args.response_fn)\n",
    "\n",
    "    # select diverse top kernels\n",
    "    selected_idxs = select_diverse(bank, responses, topM=args.topM, K=args.K, lambda_mm=args.lambda_mm)\n",
    "    print('Selected kernel indices:', selected_idxs)\n",
    "\n",
    "    # train classifier\n",
    "    clf_res = train_classifier(Xin, Xout, selected_idxs, responses, epochs=args.epochs, batch_size=args.batch_size, lr=args.lr, device=device)\n",
    "    print(f\"Classifier val AUC={clf_res['auc']:.4f} ACC={clf_res['acc']:.4f}\")\n",
    "\n",
    "    # save results\n",
    "    out = {\n",
    "        'selected_idxs': selected_idxs,\n",
    "        'bank_meta': [{'family': b['family'], 'params': b['params']} for b in bank],\n",
    "        'clf_auc': float(clf_res['auc']),\n",
    "        'clf_acc': float(clf_res['acc'])\n",
    "    }\n",
    "    np.savez_compressed(os.path.join(args.out_dir, 'results.npz'), selected_idxs=np.array(selected_idxs))\n",
    "    with open(os.path.join(args.out_dir, 'results.json'), 'w') as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "    # save kernels\n",
    "    kernels = np.stack([b['kernel'] for b in bank], axis=0)\n",
    "    np.save(os.path.join(args.out_dir, 'kernels.npy'), kernels)\n",
    "    print('Saved results to', args.out_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = SimpleNamespace(\n",
    "        data_root=OUTPUT_DIR,\n",
    "        out_dir='./results',\n",
    "        families='gaussian,anisotropic_gaussian,dog,log,gabor',\n",
    "        n_per_family=200,\n",
    "        kernel_size=31,\n",
    "        patch_size=64,\n",
    "        max_per_class=2000,\n",
    "        batch_size=64,\n",
    "        response_fn='abs_max',\n",
    "        topM=200,\n",
    "        K=20,\n",
    "        lambda_mm=0.75,\n",
    "        epochs=30,\n",
    "        lr=1e-3,\n",
    "        force_cpu=False\n",
    "    )\n",
    "\n",
    "    main(args)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 511/511 [12:50<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5277 inside patches and 3220 outside patches to data/patches\n",
      "Loaded: in=2000 out=2000 patches. Device=cuda\n",
      "Built kernel bank: 1000 kernels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kernels: 100%|██████████| 1000/1000 [00:57<00:00, 17.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected kernel indices: [340, 852, 970, 964, 938, 315, 282, 234, 252, 346, 355, 246, 325, 60, 378, 37, 385, 128, 107, 97]\n",
      "Classifier val AUC=0.9201 ACC=0.6763\n",
      "Saved results to ./results\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "6752c1f8",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_in_one",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
