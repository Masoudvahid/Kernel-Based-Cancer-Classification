{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a04a0e",
   "metadata": {},
   "source": [
    "# Kernel Model (structured)\n",
    "Refactored notebook that uses the modular `kernel_model` package instead of defining everything inline.\n",
    "Set the config values (selector = MMR or abess, optional subsampling) then run the pipeline or the cached plotting cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab35c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from kernel_model.config import (\n",
    "    KernelBankConfig,\n",
    "    PatchExtractionConfig,\n",
    "    PipelineConfig,\n",
    "    SelectionConfig,\n",
    "    SubsetSearchConfig,\n",
    "    TrainingConfig,\n",
    ")\n",
    "from kernel_model.pipeline import run_pipeline\n",
    "from kernel_model.models import fit_subset_model, find_best_low_corr_pair\n",
    "from kernel_model.selection import build_feature_matrix, rank_kernels\n",
    "from kernel_model.plots import plot_2d_scatter, plot_3d_scatter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f4955",
   "metadata": {},
   "source": [
    "## Configure the pipeline\n",
    "Adjust any defaults. By default patch extraction is skipped (expects patches under `data/patches`).\n",
    "Set `method='abess'` to use abess selection (requires `pip install abess`).\n",
    "Use `train_subset_frac` or `train_subset_size` to downsample for quicker experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b6880b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineConfig(data=PatchExtractionConfig(image_dir=PosixPath('data/TIFF Images/all'), annotation_dir=PosixPath('data/Pixel-level annotation'), output_dir=PosixPath('data/patches'), patch_size=128, n_inside_per_image=50, n_outside_per_image=100, green_threshold=150, max_tries=500, run_extraction=False), bank=KernelBankConfig(families=('gaussian', 'anisotropic_gaussian', 'dog', 'log', 'gabor'), n_per_family=200, kernel_size=31), selection=SelectionConfig(method='abess', response_fn='mean_abs', topM=200, K=20, lambda_mm=0.75, plot_top_kernels=20), training=TrainingConfig(epochs=60, batch_size=64, lr=0.0005, model_type='mlp', hidden_dims=(64, 32), dropout=0.2, standardize_features=True, force_cpu=False, train_subset_frac=None, train_subset_size=None, subset_seed=42), subset=SubsetSearchConfig(subset_search_epochs=20, boundary_epochs=25, pair_corr_threshold=0.1), data_root=PosixPath('data/patches'), out_dir=PosixPath('results'), max_per_class=2000, resize_patch_size=64, device=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = PipelineConfig(\n",
    "    data=PatchExtractionConfig(\n",
    "        image_dir=Path('data/TIFF Images/all'),\n",
    "        annotation_dir=Path('data/Pixel-level annotation'),\n",
    "        output_dir=Path('data/patches'),\n",
    "        patch_size=128,\n",
    "        n_inside_per_image=50,\n",
    "        n_outside_per_image=100,\n",
    "        green_threshold=150,\n",
    "        max_tries=500,\n",
    "        run_extraction=False,  # set True to regenerate patches\n",
    "    ),\n",
    "    bank=KernelBankConfig(),\n",
    "    selection=SelectionConfig(\n",
    "        method='abess',  # or 'mmr' (pip install abess to use abess)\n",
    "        topM=200,\n",
    "        K=20,\n",
    "    ),\n",
    "    training=TrainingConfig(\n",
    "        train_subset_frac=None,  # e.g., 0.25 to use 25% of data\n",
    "        train_subset_size=None,  # or a fixed count (overrides frac if set)\n",
    "    ),\n",
    "    subset=SubsetSearchConfig(\n",
    "        pair_corr_threshold=0.1,  # max |corr| allowed when searching best pair\n",
    "    ),\n",
    "    data_root=Path('data/patches'),\n",
    "    out_dir=Path('results'),\n",
    "    max_per_class=2000,\n",
    "    resize_patch_size=64,\n",
    ")\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a13cce",
   "metadata": {},
   "source": [
    "## Run end-to-end\n",
    "This mirrors the original notebook flow (patch load/extraction, kernel bank, responses, selection, training, plots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf8b195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:15:54,806 [INFO] Starting pipeline. Output: results/exp_005 Device: cuda Selection: abess\n",
      "2025-12-06 16:15:56,639 [INFO] Loaded patches Xin=2000 Xout=2000 with resize=64\n",
      "2025-12-06 16:15:56,724 [INFO] Built kernel bank: 1000 kernels\n",
      "Kernels: 100%|██████████| 1000/1000 [00:56<00:00, 17.58it/s]\n",
      "2025-12-06 16:16:55,303 [INFO] abess selected kernels (candidate idx): [105, 121, 150, 152, 154, 156, 158, 166, 184, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 199]\n",
      "2025-12-06 16:17:01,309 [INFO] Classifier results: AUC=0.9803 ACC=0.9237\n",
      "2025-12-06 17:02:20,980 [INFO] No pair met |corr|<=0.100; best available (min corr=0.9941) kernels [913, 291] (cols (1, 17)) AUC=0.9820 ACC=0.9350 Corr=0.9941\n",
      "2025-12-06 17:02:20,980 [INFO] Best pair (plot candidate idx): (1, 19) AUC=0.9820\n",
      "2025-12-06 17:02:23,383 [INFO] Best triple (plot candidate idx): (0, 1, 6) AUC=0.9823\n",
      "2025-12-06 17:02:23,712 [INFO] Saved artifacts to results/exp_005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'config': PipelineConfig(data=PatchExtractionConfig(image_dir=PosixPath('data/TIFF Images/all'), annotation_dir=PosixPath('data/Pixel-level annotation'), output_dir=PosixPath('data/patches'), patch_size=128, n_inside_per_image=50, n_outside_per_image=100, green_threshold=150, max_tries=500, run_extraction=False), bank=KernelBankConfig(families=('gaussian', 'anisotropic_gaussian', 'dog', 'log', 'gabor'), n_per_family=200, kernel_size=31), selection=SelectionConfig(method='abess', response_fn='mean_abs', topM=200, K=20, lambda_mm=0.75, plot_top_kernels=20), training=TrainingConfig(epochs=60, batch_size=64, lr=0.0005, model_type='mlp', hidden_dims=(64, 32), dropout=0.2, standardize_features=True, force_cpu=False, train_subset_frac=None, train_subset_size=None, subset_seed=42), subset=SubsetSearchConfig(subset_search_epochs=20, boundary_epochs=25, pair_corr_threshold=0.1), data_root=PosixPath('data/patches'), out_dir=PosixPath('results/exp_005'), max_per_class=2000, resize_patch_size=64, device=None),\n",
       " 'device': 'cuda',\n",
       " 'clf': {'model': SimpleMLP(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=20, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=32, out_features=1, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'auc': 0.9802749999999998,\n",
       "  'acc': 0.92375,\n",
       "  'val_probs': array([9.7641039e-01, 9.8602390e-01, 8.4324151e-01, 9.9770725e-01,\n",
       "         9.3566614e-01, 9.7613668e-01, 1.2840440e-04, 9.7328150e-01,\n",
       "         1.2840440e-04, 2.7689186e-01, 2.9721692e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 1.2840440e-04, 1.2840440e-04, 2.5191468e-01,\n",
       "         6.2539417e-01, 2.6999184e-01, 9.6822727e-01, 1.2840440e-04,\n",
       "         2.6944271e-01, 9.2968261e-01, 1.2840440e-04, 2.4814984e-01,\n",
       "         9.5683545e-01, 1.3558826e-04, 9.6030247e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 2.5290787e-01, 1.2840440e-04, 9.7384381e-01,\n",
       "         9.9297440e-01, 9.6578193e-01, 9.6637660e-01, 2.4364960e-01,\n",
       "         9.5166928e-01, 1.2840440e-04, 9.7327310e-01, 8.2935554e-01,\n",
       "         9.7563213e-01, 7.5761163e-01, 2.5467649e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 9.7498804e-01, 1.2840440e-04, 1.2840440e-04,\n",
       "         1.2840440e-04, 9.4217193e-01, 9.3170190e-01, 8.8889265e-01,\n",
       "         9.6171904e-01, 9.5231211e-01, 2.8875345e-01, 9.9799901e-01,\n",
       "         1.2840440e-04, 1.2840440e-04, 9.1394049e-01, 1.2840697e-04,\n",
       "         1.2843245e-04, 1.2840440e-04, 9.8763925e-01, 1.2840440e-04,\n",
       "         2.6226419e-01, 1.2840440e-04, 1.2840440e-04, 1.2840440e-04,\n",
       "         3.0488953e-01, 7.4004531e-01, 1.2840440e-04, 1.2841653e-04,\n",
       "         1.2840440e-04, 9.0551227e-01, 9.8744941e-01, 1.2840440e-04,\n",
       "         9.8064202e-01, 2.4160926e-01, 2.4880533e-01, 1.2840440e-04,\n",
       "         8.3143938e-01, 9.8363471e-01, 1.2899273e-04, 1.2840440e-04,\n",
       "         7.7568930e-01, 1.2842890e-04, 1.2840440e-04, 9.5398861e-01,\n",
       "         9.2859018e-01, 7.2045809e-01, 9.9275655e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 2.4403243e-01, 2.4816428e-01, 2.4066199e-01,\n",
       "         7.4294591e-01, 1.2840440e-04, 1.2840440e-04, 1.2840440e-04,\n",
       "         1.4096412e-01, 9.7490364e-01, 2.4069324e-01, 8.9550585e-01,\n",
       "         9.1652232e-01, 2.4665070e-01, 1.2840440e-04, 1.2840440e-04,\n",
       "         9.7100741e-01, 1.2840440e-04, 1.2840440e-04, 9.6961457e-01,\n",
       "         2.4347928e-01, 9.6048707e-01, 8.6545527e-01, 9.7967392e-01,\n",
       "         9.9084049e-01, 1.2840440e-04, 1.2840440e-04, 2.4042693e-01,\n",
       "         3.7992162e-01, 9.0896690e-01, 9.8038381e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 9.4095057e-01, 9.7907329e-01, 1.2840440e-04,\n",
       "         1.2841531e-04, 9.0391552e-01, 1.2840440e-04, 9.5010883e-01,\n",
       "         8.2661515e-01, 1.2840440e-04, 1.2840440e-04, 9.5856518e-01,\n",
       "         9.3018633e-01, 9.2271310e-01, 2.5037795e-01, 9.7880906e-01,\n",
       "         1.2840440e-04, 1.2840440e-04, 1.2840440e-04, 9.8334759e-01,\n",
       "         9.2367363e-01, 1.2840440e-04, 7.8623188e-01, 7.9783440e-01,\n",
       "         2.9674178e-01, 9.6449226e-01, 2.6855478e-01, 1.2840440e-04,\n",
       "         2.4495710e-01, 9.5108783e-01, 1.2840440e-04, 9.8226017e-01,\n",
       "         8.8996255e-01, 9.9636441e-01, 9.8096919e-01, 9.1007024e-01,\n",
       "         9.2884701e-01, 9.9478662e-01, 1.2840440e-04, 1.2840440e-04,\n",
       "         3.6675936e-01, 1.2840440e-04, 9.2503822e-01, 2.9324409e-01,\n",
       "         1.2840440e-04, 1.2840440e-04, 1.2840440e-04, 1.2840440e-04,\n",
       "         9.7768438e-01, 1.2843245e-04, 1.2840440e-04, 9.5947772e-01,\n",
       "         1.2840440e-04, 1.2840440e-04, 9.7702754e-01, 1.2840440e-04,\n",
       "         3.0601561e-01, 9.6893030e-01, 1.2840440e-04, 1.2840440e-04,\n",
       "         1.2840440e-04, 9.8401457e-01, 9.6751273e-01, 9.7580701e-01,\n",
       "         1.2840440e-04, 9.1471869e-01, 9.9235338e-01, 1.2840440e-04,\n",
       "         8.4745854e-01, 9.6288753e-01, 1.2840440e-04, 9.7378403e-01,\n",
       "         1.2840440e-04, 2.4057956e-01, 9.0627927e-01, 7.6838732e-01,\n",
       "         9.8149097e-01, 9.4804001e-01, 2.4815448e-01, 1.2840440e-04,\n",
       "         9.5008951e-01, 4.6917254e-01, 1.2840440e-04, 9.7603577e-01,\n",
       "         1.2840440e-04, 7.0977437e-01, 6.7974389e-01, 9.6703053e-01,\n",
       "         9.8452330e-01, 9.4586647e-01, 1.2840440e-04, 9.9549353e-01,\n",
       "         1.2842412e-04, 8.8654160e-01, 9.8885369e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 9.6574044e-01, 2.4511309e-01, 1.2863149e-04,\n",
       "         1.2840440e-04, 1.2840440e-04, 9.9169946e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 9.2581069e-01, 1.2840440e-04, 9.6102118e-01,\n",
       "         1.2840440e-04, 9.5851219e-01, 3.0417192e-01, 1.2840440e-04,\n",
       "         2.4645504e-01, 1.2840440e-04, 1.2840440e-04, 9.5321453e-01,\n",
       "         5.7288873e-01, 9.9564040e-01, 1.2840440e-04, 9.8214781e-01,\n",
       "         4.6216384e-01, 5.6377435e-01, 9.9446929e-01, 9.8591214e-01,\n",
       "         2.5333476e-01, 1.2840440e-04, 9.5877105e-01, 9.6360397e-01,\n",
       "         9.5569521e-01, 9.3668133e-01, 1.2840440e-04, 1.2840440e-04,\n",
       "         4.1189584e-01, 2.3756474e-01, 9.3333149e-01, 2.0700991e-01,\n",
       "         2.4073352e-01, 8.5079604e-01, 9.9806601e-01, 8.4600890e-01,\n",
       "         8.3630073e-01, 1.2840440e-04, 4.6615748e-04, 6.7042154e-01,\n",
       "         9.7410721e-01, 1.2840440e-04, 1.2840440e-04, 1.2840440e-04,\n",
       "         9.4773251e-01, 9.2803103e-01, 1.2840440e-04, 9.8501366e-01,\n",
       "         1.2842852e-04, 2.4149069e-01, 1.2840440e-04, 9.8765135e-01,\n",
       "         9.8382944e-01, 9.5777905e-01, 2.5514147e-01, 9.6055728e-01,\n",
       "         9.4106627e-01, 9.7882986e-01, 1.2840440e-04, 9.0111029e-01,\n",
       "         5.4653680e-01, 9.8704499e-01, 1.2843404e-04, 4.1685396e-04,\n",
       "         5.3629464e-01, 9.9286085e-01, 9.7900599e-01, 9.7322583e-01,\n",
       "         2.9400027e-01, 9.6618015e-01, 2.4709161e-01, 9.8679060e-01,\n",
       "         5.8563620e-01, 1.3187299e-04, 1.2840440e-04, 9.7325045e-01,\n",
       "         1.2840440e-04, 1.2840440e-04, 1.2840440e-04, 1.2840440e-04,\n",
       "         9.9705827e-01, 1.2840440e-04, 1.2840440e-04, 2.7295312e-01,\n",
       "         9.9177915e-01, 5.1038492e-01, 1.2840440e-04, 6.5430021e-01,\n",
       "         9.0533119e-01, 9.7256529e-01, 1.2840440e-04, 1.3181039e-04,\n",
       "         9.7707486e-01, 8.3458757e-01, 9.8902124e-01, 2.4194409e-01,\n",
       "         9.3999022e-01, 9.1814822e-01, 1.2840440e-04, 1.2844322e-04,\n",
       "         9.9476236e-01, 1.2840440e-04, 1.2840440e-04, 5.3855896e-01,\n",
       "         9.4323599e-01, 9.4005960e-01, 5.6583548e-01, 3.0238277e-01,\n",
       "         9.9726701e-01, 7.2946352e-01, 9.7544515e-01, 6.0757780e-01,\n",
       "         9.8298806e-01, 9.9799466e-01, 1.2840440e-04, 9.4776505e-01,\n",
       "         9.7286296e-01, 1.2840440e-04, 9.6294737e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 9.4300026e-01, 3.0276600e-01, 9.8873454e-01,\n",
       "         9.8627627e-01, 1.2840440e-04, 9.7654378e-01, 2.8500226e-01,\n",
       "         9.9060678e-01, 9.1568851e-01, 9.3116152e-01, 9.5923895e-01,\n",
       "         2.5506824e-01, 9.4908279e-01, 9.6235102e-01, 9.3857092e-01,\n",
       "         1.2848218e-04, 9.8269826e-01, 9.4816089e-01, 1.2840440e-04,\n",
       "         9.3954718e-01, 5.3999317e-01, 9.6513188e-01, 1.2840440e-04,\n",
       "         8.5143179e-01, 2.4067982e-01, 9.8538798e-01, 1.2840440e-04,\n",
       "         9.8045588e-01, 1.2840440e-04, 9.4745177e-01, 9.5022100e-01,\n",
       "         1.2840440e-04, 2.4200845e-01, 2.4203652e-01, 2.4216603e-01,\n",
       "         1.2840440e-04, 1.2840440e-04, 1.2840440e-04, 1.2840440e-04,\n",
       "         4.6660823e-01, 9.6974337e-01, 9.7321665e-01, 2.4720906e-01,\n",
       "         2.5659618e-01, 3.0599150e-01, 9.8957890e-01, 1.2840440e-04,\n",
       "         9.9491960e-01, 8.3115411e-01, 1.2840440e-04, 1.2840440e-04,\n",
       "         9.7739875e-01, 7.1914726e-01, 9.6974283e-01, 3.0499408e-01,\n",
       "         9.8556906e-01, 2.9976848e-01, 9.3251455e-01, 9.8474991e-01,\n",
       "         1.2840440e-04, 2.5274450e-01, 9.9731207e-01, 9.4085145e-01,\n",
       "         9.7565526e-01, 1.2840440e-04, 1.2840440e-04, 9.9476904e-01,\n",
       "         1.2840440e-04, 1.2840440e-04, 9.7691262e-01, 2.4943154e-01,\n",
       "         1.2840440e-04, 1.2840440e-04, 9.4657403e-01, 2.4450624e-01,\n",
       "         1.2841262e-04, 8.3546221e-01, 9.4431543e-01, 2.4095491e-01,\n",
       "         9.8411936e-01, 1.2864510e-04, 9.7867626e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 9.8564106e-01, 1.2840440e-04, 1.2840440e-04,\n",
       "         1.2840440e-04, 9.6159029e-01, 8.3491015e-01, 4.6802914e-01,\n",
       "         2.5811690e-01, 7.6127267e-01, 1.2840440e-04, 1.2840440e-04,\n",
       "         9.0800786e-01, 9.7158957e-01, 8.5147083e-01, 8.4381104e-01,\n",
       "         9.8960900e-01, 9.9411809e-01, 9.1577029e-01, 9.5467347e-01,\n",
       "         2.7733067e-01, 1.2840440e-04, 1.2848745e-04, 1.2840440e-04,\n",
       "         1.2840440e-04, 1.2840440e-04, 2.4477100e-01, 2.4104652e-01,\n",
       "         6.8173081e-01, 1.2840440e-04, 1.2840440e-04, 1.2840440e-04,\n",
       "         5.0506544e-01, 1.2840440e-04, 2.4064921e-01, 9.8565674e-01,\n",
       "         9.2126548e-01, 9.6094567e-01, 1.2840440e-04, 8.9121664e-01,\n",
       "         2.8366113e-01, 2.5149712e-01, 9.9475455e-01, 4.3034676e-01,\n",
       "         9.7823626e-01, 1.2840440e-04, 8.6173758e-02, 1.2840440e-04,\n",
       "         2.4576217e-01, 2.4348474e-01, 9.4398582e-01, 9.6993357e-01,\n",
       "         1.2840440e-04, 8.9202023e-01, 3.0250725e-01, 3.9109431e-02,\n",
       "         1.2840440e-04, 1.2840440e-04, 9.5308334e-01, 9.7625935e-01,\n",
       "         8.9627790e-01, 9.5367312e-01, 9.8068237e-01, 7.1644068e-01,\n",
       "         9.4556159e-01, 9.9666542e-01, 9.2283165e-01, 1.2840440e-04,\n",
       "         9.9051595e-01, 9.9527788e-01, 9.9780291e-01, 9.5352536e-01,\n",
       "         1.2840440e-04, 1.2840440e-04, 9.7252965e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 6.5537393e-01, 9.1614383e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 2.5074783e-01, 2.4133702e-01, 2.8507641e-01,\n",
       "         1.2840440e-04, 9.8971808e-01, 1.2840440e-04, 6.5303969e-01,\n",
       "         1.2840440e-04, 1.2840440e-04, 3.0047208e-01, 9.9571323e-01,\n",
       "         2.5886962e-01, 1.2840440e-04, 9.5043075e-01, 9.3072659e-01,\n",
       "         9.5785660e-01, 1.2840440e-04, 1.2840440e-04, 1.2840440e-04,\n",
       "         1.2851943e-04, 9.5308715e-01, 1.2840440e-04, 8.5319465e-01,\n",
       "         9.3728417e-01, 1.2840440e-04, 9.6214437e-01, 9.9537873e-01,\n",
       "         9.6377844e-01, 9.8739094e-01, 1.3029024e-04, 7.7211320e-01,\n",
       "         8.6076432e-01, 2.6384035e-01, 2.9626283e-01, 8.5576576e-01,\n",
       "         1.2840440e-04, 6.5491831e-01, 9.5255595e-01, 8.9200819e-01,\n",
       "         8.6503839e-01, 2.9410633e-01, 1.2840440e-04, 1.2840440e-04,\n",
       "         2.4819019e-01, 9.8786527e-01, 9.5996118e-01, 9.8580223e-01,\n",
       "         1.2840440e-04, 9.8241132e-01, 1.2841262e-04, 1.2840440e-04,\n",
       "         1.2840440e-04, 9.4726139e-01, 9.2233843e-01, 2.4054150e-01,\n",
       "         9.6358937e-01, 9.5615816e-01, 1.2840440e-04, 1.2840440e-04,\n",
       "         3.2519975e-01, 1.2840440e-04, 1.2840440e-04, 1.2840440e-04,\n",
       "         9.4863319e-01, 2.4160893e-01, 9.6120673e-01, 1.2840440e-04,\n",
       "         9.5123017e-01, 3.3068976e-01, 8.9991063e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 9.5922458e-01, 1.2840440e-04, 1.2840440e-04,\n",
       "         9.8857343e-01, 9.7028625e-01, 9.7809780e-01, 1.3692833e-04,\n",
       "         9.2404854e-01, 1.6924368e-02, 9.8735005e-01, 9.7985429e-01,\n",
       "         1.2840440e-04, 9.5064229e-01, 9.9717307e-01, 2.4174601e-01,\n",
       "         1.2840440e-04, 9.7800648e-01, 1.2840440e-04, 9.7084594e-01,\n",
       "         9.8734134e-01, 1.2850840e-04, 1.2841763e-04, 1.2840440e-04,\n",
       "         1.2840440e-04, 9.8350269e-01, 8.2842773e-01, 9.6114683e-01,\n",
       "         1.2840440e-04, 1.2840440e-04, 9.7628945e-01, 2.7240285e-01,\n",
       "         8.9737177e-01, 9.4904888e-01, 9.4302404e-01, 8.7193823e-01,\n",
       "         1.2840440e-04, 1.2840440e-04, 1.2840440e-04, 8.4538537e-01,\n",
       "         9.9286312e-01, 7.5948733e-01, 9.8141348e-01, 9.8152542e-01,\n",
       "         6.8749261e-01, 9.9177259e-01, 2.4265684e-01, 9.7786582e-01,\n",
       "         1.2840440e-04, 9.9092603e-01, 1.2840440e-04, 9.6844357e-01,\n",
       "         1.2840440e-04, 1.2840440e-04, 1.2840440e-04, 3.0707267e-01,\n",
       "         1.2840440e-04, 4.4154471e-01, 9.6460962e-01, 9.8545408e-01,\n",
       "         1.2840440e-04, 7.1481943e-01, 5.8587605e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 1.2840440e-04, 2.8990933e-01, 6.7713487e-01,\n",
       "         9.9424875e-01, 1.2840440e-04, 8.1115901e-01, 7.1414667e-01,\n",
       "         1.2840440e-04, 9.7038120e-01, 1.2840440e-04, 9.8277724e-01,\n",
       "         9.8139602e-01, 7.5452566e-01, 1.2840440e-04, 2.4189441e-01,\n",
       "         9.7914565e-01, 9.8581785e-01, 9.8938012e-01, 9.3869370e-01,\n",
       "         9.9427080e-01, 3.2517916e-01, 9.9449843e-01, 1.2840440e-04,\n",
       "         6.5292233e-01, 4.4738489e-01, 2.5779364e-01, 9.9755931e-01,\n",
       "         8.8192588e-01, 8.8812757e-01, 9.1709900e-01, 9.5557719e-01,\n",
       "         1.3519540e-04, 2.8197258e-03, 2.6336202e-01, 2.6822394e-01,\n",
       "         8.9897376e-01, 9.9462849e-01, 2.8878284e-01, 9.8270386e-01,\n",
       "         9.7577387e-01, 1.2840440e-04, 9.4388467e-01, 9.9457979e-01,\n",
       "         1.2840440e-04, 2.7917042e-01, 9.8039985e-01, 1.2840440e-04,\n",
       "         1.4979042e-04, 1.2840440e-04, 1.2840440e-04, 9.6139580e-01,\n",
       "         1.3287051e-04, 3.1211796e-01, 1.2840440e-04, 9.8154587e-01,\n",
       "         1.2840440e-04, 3.8483524e-01, 9.7892958e-01, 9.7203875e-01,\n",
       "         1.2840440e-04, 2.4157937e-01, 9.9521130e-01, 3.4767172e-01,\n",
       "         1.2840440e-04, 9.9245363e-01, 1.2840440e-04, 1.2840440e-04,\n",
       "         1.2840440e-04, 1.2840440e-04, 9.7020578e-01, 7.9476064e-01,\n",
       "         9.2741513e-01, 2.6433414e-01, 1.2840440e-04, 1.2840440e-04,\n",
       "         9.8154837e-01, 5.4123557e-01, 9.6447653e-01, 9.3928283e-01,\n",
       "         2.5664124e-01, 9.8801476e-01, 1.2840440e-04, 9.7277260e-01,\n",
       "         2.4230094e-01, 1.2841543e-04, 9.8794252e-01, 9.4880104e-01,\n",
       "         8.8006359e-01, 9.4264847e-01, 4.1980481e-01, 9.4365299e-01,\n",
       "         1.2840440e-04, 4.6152639e-01, 1.2840440e-04, 8.1834203e-01,\n",
       "         9.9585092e-01, 1.2840440e-04, 9.7318125e-01, 1.2840440e-04,\n",
       "         9.2312533e-01, 2.4702495e-01, 9.9697423e-01, 9.2808974e-01,\n",
       "         1.2840440e-04, 8.8183779e-01, 1.2840440e-04, 9.8975414e-01,\n",
       "         1.2840440e-04, 7.3758894e-01, 1.2840440e-04, 9.8005861e-01,\n",
       "         1.2840440e-04, 1.3834955e-04, 9.9060553e-01, 1.2840440e-04,\n",
       "         1.2840440e-04, 9.1866684e-01, 9.9486005e-01, 2.8497460e-01,\n",
       "         9.9386567e-01, 1.2840440e-04, 1.2840440e-04, 1.2840440e-04,\n",
       "         2.5257760e-01, 1.3582247e-04, 1.2840440e-04, 1.2840440e-04,\n",
       "         6.4651656e-01, 9.6853906e-01, 2.4922664e-01, 3.8016126e-01,\n",
       "         2.9996639e-01, 9.8360705e-01, 2.4244347e-01, 3.0576366e-01,\n",
       "         2.9627004e-01, 1.2840440e-04, 7.6644593e-01, 9.6197724e-01,\n",
       "         8.5920531e-01, 9.3711066e-01, 2.6241067e-01, 1.2847115e-04,\n",
       "         1.2849970e-04, 1.2840440e-04, 9.6736544e-01, 2.4516125e-01,\n",
       "         1.2847164e-04, 2.4080981e-01, 1.2840440e-04, 9.3388844e-01],\n",
       "        dtype=float32),\n",
       "  'val_labels': array([1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "         0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
       "         1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "         0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "         1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "         0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "         1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "         1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "         1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
       "         0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "         1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "         1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "         0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "         0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "         0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
       "         1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
       "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "         1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "         1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "         1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "         0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "         1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "         0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "         0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "         1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "         1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "         1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
       "         1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         1.])},\n",
       " 'selected_idxs': [343,\n",
       "  305,\n",
       "  322,\n",
       "  375,\n",
       "  246,\n",
       "  241,\n",
       "  234,\n",
       "  329,\n",
       "  170,\n",
       "  341,\n",
       "  239,\n",
       "  395,\n",
       "  3,\n",
       "  76,\n",
       "  64,\n",
       "  75,\n",
       "  200,\n",
       "  353,\n",
       "  124,\n",
       "  81],\n",
       " 'subset_results': {2: {'subset': (1, 19), 'auc': 0.98203125, 'acc': 0.9325},\n",
       "  3: {'subset': (0, 1, 6), 'auc': 0.9823312499999999, 'acc': 0.93375}},\n",
       " 'low_corr_pair': {'subset': (1, 17),\n",
       "  'kernel_idxs': [913, 291],\n",
       "  'auc': 0.9819749999999999,\n",
       "  'acc': 0.935,\n",
       "  'corr': 0.9941486120223999,\n",
       "  'abs_corr': 0.9941486120223999,\n",
       "  'threshold_met': False,\n",
       "  'model': None},\n",
       " 'candidate_kernel_idxs': [803,\n",
       "  913,\n",
       "  258,\n",
       "  267,\n",
       "  28,\n",
       "  160,\n",
       "  109,\n",
       "  15,\n",
       "  386,\n",
       "  129,\n",
       "  178,\n",
       "  25,\n",
       "  119,\n",
       "  149,\n",
       "  186,\n",
       "  150,\n",
       "  7,\n",
       "  291,\n",
       "  1,\n",
       "  50,\n",
       "  185,\n",
       "  172,\n",
       "  180,\n",
       "  98,\n",
       "  152,\n",
       "  276,\n",
       "  300,\n",
       "  108,\n",
       "  144,\n",
       "  288,\n",
       "  383,\n",
       "  226,\n",
       "  230,\n",
       "  385,\n",
       "  351,\n",
       "  141,\n",
       "  8,\n",
       "  248,\n",
       "  251,\n",
       "  73,\n",
       "  237,\n",
       "  38,\n",
       "  308,\n",
       "  349,\n",
       "  68,\n",
       "  169,\n",
       "  47,\n",
       "  112,\n",
       "  128,\n",
       "  24,\n",
       "  296,\n",
       "  369,\n",
       "  282,\n",
       "  4,\n",
       "  16,\n",
       "  51,\n",
       "  148,\n",
       "  359,\n",
       "  265,\n",
       "  184,\n",
       "  78,\n",
       "  84,\n",
       "  130,\n",
       "  163,\n",
       "  242,\n",
       "  382,\n",
       "  394,\n",
       "  65,\n",
       "  221,\n",
       "  41,\n",
       "  333,\n",
       "  49,\n",
       "  59,\n",
       "  120,\n",
       "  18,\n",
       "  293,\n",
       "  36,\n",
       "  261,\n",
       "  138,\n",
       "  158,\n",
       "  142,\n",
       "  245,\n",
       "  362,\n",
       "  177,\n",
       "  220,\n",
       "  60,\n",
       "  264,\n",
       "  217,\n",
       "  263,\n",
       "  306,\n",
       "  313,\n",
       "  321,\n",
       "  334,\n",
       "  368,\n",
       "  0,\n",
       "  159,\n",
       "  378,\n",
       "  111,\n",
       "  34,\n",
       "  255,\n",
       "  52,\n",
       "  311,\n",
       "  194,\n",
       "  380,\n",
       "  45,\n",
       "  343,\n",
       "  336,\n",
       "  33,\n",
       "  40,\n",
       "  26,\n",
       "  107,\n",
       "  135,\n",
       "  188,\n",
       "  37,\n",
       "  190,\n",
       "  54,\n",
       "  117,\n",
       "  252,\n",
       "  48,\n",
       "  238,\n",
       "  82,\n",
       "  305,\n",
       "  301,\n",
       "  2,\n",
       "  104,\n",
       "  102,\n",
       "  271,\n",
       "  56,\n",
       "  290,\n",
       "  157,\n",
       "  32,\n",
       "  182,\n",
       "  11,\n",
       "  280,\n",
       "  216,\n",
       "  202,\n",
       "  326,\n",
       "  320,\n",
       "  191,\n",
       "  325,\n",
       "  344,\n",
       "  22,\n",
       "  363,\n",
       "  390,\n",
       "  86,\n",
       "  316,\n",
       "  331,\n",
       "  176,\n",
       "  244,\n",
       "  257,\n",
       "  322,\n",
       "  83,\n",
       "  375,\n",
       "  10,\n",
       "  246,\n",
       "  99,\n",
       "  241,\n",
       "  358,\n",
       "  234,\n",
       "  302,\n",
       "  113,\n",
       "  147,\n",
       "  12,\n",
       "  100,\n",
       "  309,\n",
       "  212,\n",
       "  329,\n",
       "  97,\n",
       "  136,\n",
       "  193,\n",
       "  90,\n",
       "  121,\n",
       "  9,\n",
       "  222,\n",
       "  192,\n",
       "  29,\n",
       "  256,\n",
       "  66,\n",
       "  338,\n",
       "  229,\n",
       "  381,\n",
       "  137,\n",
       "  171,\n",
       "  314,\n",
       "  170,\n",
       "  224,\n",
       "  341,\n",
       "  239,\n",
       "  398,\n",
       "  395,\n",
       "  3,\n",
       "  76,\n",
       "  64,\n",
       "  75,\n",
       "  200,\n",
       "  353,\n",
       "  124,\n",
       "  337,\n",
       "  93,\n",
       "  81],\n",
       " 'plot_candidate_idxs': [803,\n",
       "  913,\n",
       "  258,\n",
       "  267,\n",
       "  28,\n",
       "  160,\n",
       "  109,\n",
       "  15,\n",
       "  386,\n",
       "  129,\n",
       "  178,\n",
       "  25,\n",
       "  119,\n",
       "  149,\n",
       "  186,\n",
       "  150,\n",
       "  7,\n",
       "  291,\n",
       "  1,\n",
       "  50],\n",
       " 'selection_method': 'abess',\n",
       " 'out_dir': PosixPath('results/exp_005')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = run_pipeline(cfg)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96ce0b",
   "metadata": {},
   "source": [
    "## Re-plot from cached features\n",
    "Use `results/feature_cache.npz` to redraw the best pair/triple without recomputing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eccd44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote interactive plot to scatter_best_triple_interactive.html\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from kernel_model.plots import plot_3d_scatter_interactive\n",
    "import numpy as np\n",
    "cfg.out_dir = Path(\"/home/masoud/Uni/Kernel Based Cancer Classification/results/exp_005/\")\n",
    "\n",
    "cache = np.load(cfg.out_dir / \"feature_cache.npz\", allow_pickle=True)\n",
    "X_candidates = cache[\"X_candidates\"]\n",
    "y_labels = cache[\"y_labels\"]\n",
    "subset_results = cache[\"subset_results\"].item()\n",
    "candidate_kernel_idxs = cache[\"candidate_kernel_idxs\"]\n",
    "\n",
    "if 3 in subset_results:\n",
    "    triple = subset_results[3]\n",
    "    triple_kernel_idxs = [candidate_kernel_idxs[i] for i in triple[\"subset\"]]\n",
    "    plot_3d_scatter_interactive(\n",
    "        X_candidates,\n",
    "        y_labels,\n",
    "        triple[\"subset\"],\n",
    "        triple_kernel_idxs,\n",
    "        cfg.out_dir / \"scatter_best_triple_interactive_CancerProjected.html\",\n",
    "        project_cancer_to_plane=True,\n",
    "        project_healthy_to_plane=True,\n",
    "    )\n",
    "    print(\"Wrote interactive plot to scatter_best_triple_interactive.html\")\n",
    "else:\n",
    "    print(\"No triple subset in cache\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b1f54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No low-corr pair found\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "cache = np.load(cfg.out_dir / 'feature_cache.npz', allow_pickle=True)\n",
    "X_candidates = cache['X_candidates']\n",
    "y_labels = cache['y_labels']\n",
    "subset_results = cache['subset_results'].item()\n",
    "candidate_kernel_idxs = cache['candidate_kernel_idxs']\n",
    "\n",
    "low_corr_pair = result.get('low_corr_pair') if 'result' in globals() else None\n",
    "results_json = cfg.out_dir / 'results.json'\n",
    "if low_corr_pair is None and results_json.exists():\n",
    "    low_corr_pair = json.load(open(results_json)).get('low_corr_pair')\n",
    "if low_corr_pair is None:\n",
    "    low_corr_pair = find_best_low_corr_pair(\n",
    "        X_candidates,\n",
    "        y_labels,\n",
    "        candidate_kernel_idxs,\n",
    "        corr_threshold=cfg.subset.pair_corr_threshold,\n",
    "        epochs=10,\n",
    "        batch_size=cfg.training.batch_size,\n",
    "        lr=cfg.training.lr,\n",
    "        device='cpu',\n",
    "        model_type=cfg.training.model_type,\n",
    "        hidden_dims=cfg.training.hidden_dims,\n",
    "        dropout=cfg.training.dropout,\n",
    "        standardize=cfg.training.standardize_features,\n",
    "        refit_best=False,\n",
    "    )\n",
    "if low_corr_pair and low_corr_pair.get('subset') is not None:\n",
    "    status = 'within threshold' if low_corr_pair.get('threshold_met') else 'fallback (no pair met threshold)'\n",
    "    print(f\"Low-corr pair [{status}]: kernels {low_corr_pair['kernel_idxs']} cols {low_corr_pair['subset']} corr={low_corr_pair['corr']:.4f} auc={low_corr_pair['auc']:.4f}\")\n",
    "else:\n",
    "    print('No low-corr pair found')\n",
    "\n",
    "if 2 in subset_results:\n",
    "    pair = subset_results[2]\n",
    "    pair_kernel_idxs = [candidate_kernel_idxs[i] for i in pair['subset']]\n",
    "    boundary_model = fit_subset_model(\n",
    "        X_candidates,\n",
    "        y_labels,\n",
    "        pair['subset'],\n",
    "        epochs=20,\n",
    "        batch_size=cfg.training.batch_size,\n",
    "        lr=cfg.training.lr,\n",
    "        device='cpu',\n",
    "        model_type=cfg.training.model_type,\n",
    "        hidden_dims=cfg.training.hidden_dims,\n",
    "        dropout=cfg.training.dropout,\n",
    "        standardize=cfg.training.standardize_features,\n",
    "    )\n",
    "    plot_2d_scatter(\n",
    "        X_candidates,\n",
    "        y_labels,\n",
    "        pair['subset'],\n",
    "        pair_kernel_idxs,\n",
    "        cfg.out_dir / 'scatter_best_pair_cached.png',\n",
    "        boundary=boundary_model,\n",
    "        title='Best 2-kernel feature space (cached)',\n",
    "    )\n",
    "\n",
    "if 3 in subset_results:\n",
    "    triple = subset_results[3]\n",
    "    triple_kernel_idxs = [candidate_kernel_idxs[i] for i in triple['subset']]\n",
    "    plot_3d_scatter(\n",
    "        X_candidates, y_labels, triple['subset'], triple_kernel_idxs, cfg.out_dir / 'scatter_best_triple_cached.png'\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_in_one",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
